{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "* [Github repo](https://github.com/hwchase17/langchain)\n",
    "* [Python documentation](https://python.langchain.com/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* See https://beta.openai.com/account/api-keys to get your API key\n",
    "* Follow https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety to set up the OPENAI_API_KEY environment variable (used in the code below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Azure Open AI API\n",
    "\n",
    "* https://python.langchain.com/en/latest/modules/models/llms/integrations/azure_openai_example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM models\n",
    "LangChain provides a consistent interface to large language models (various types, various providers, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Completion API\n",
    "* E.g. text-davinci-003\n",
    "* Mode = text-in-text-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A dish associated with Pune is Paani Puri, a type of chaat (a savory snack) consisting of fried, hollow puri (an unleavened deep-fried Indian bread) filled with potatoes, onions, chutney and a spicy flavored water (paani).\n"
     ]
    }
   ],
   "source": [
    "# Completion API (text-davinci-003): text-in-text-out\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name='text-davinci-003',\n",
    "             temperature=0.9,\n",
    "             openai_api_key=openai_api_key,\n",
    "             max_tokens=1024,\n",
    "             top_p=1)\n",
    "\n",
    "prompt = \"What is a dish associated with Pune?\"\n",
    "\n",
    "response = llm(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chat API\n",
    "* E.g. gpt-3.5-turbo\n",
    "* Mode = list-of-messages-in-message-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Misal Pav is a popular dish associated with Pune, Maharashtra.', additional_kwargs={})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
    "                  temperature=0.9,\n",
    "                  openai_api_key=openai_api_key)\n",
    "\n",
    "response = chat(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an AI assistant that helps a user learn about food. Keep responses short and under 2 sentences.\"),\n",
    "        HumanMessage(content=\"What is a dish associated with Pune?\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Embeddings\n",
    "* E.g. text-embedding-ada-002\n",
    "* Mode = text-in-vector-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1536\n",
      "[0.007298551898297813, -0.010412962080020289, 0.011232186936721102, -0.0041028963955430245, -0.00617804139751549]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "openai_embed = OpenAIEmbeddings(model='text-embedding-ada-002',\n",
    "                                openai_api_key=openai_api_key)\n",
    "\n",
    "query_embeddings = openai_embed.embed_query(\"Vada Pav\") \n",
    "\n",
    "print(type(query_embeddings))\n",
    "print(len(query_embeddings))\n",
    "print(query_embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate\n",
    "Allows creating the final prompt from a template using user input and other non-static information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL PROMPT: What is a dish most associated with Pune?\n",
      "\n",
      "\n",
      "Pav Bhaji is a popular dish from Pune. The dish is a combination of mashed vegetables, potatoes and spices served with a lightly toasted bun (or pav).\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt template with placeholders for input variables.\n",
    "prompt_template = PromptTemplate(\n",
    "        template = 'What is a {reco_type} most associated with {location}?',\n",
    "        input_variables = ['reco_type', 'location']\n",
    ")\n",
    "\n",
    "# Create the final prompt by filling in the defined variables.\n",
    "final_prompt = prompt_template.format(reco_type='dish',  location='Pune')\n",
    "print(\"FINAL PROMPT: \" + final_prompt)\n",
    "\n",
    "response = llm(final_prompt) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hyderabadi Biryani is the most popular and iconic dish associated with Hyderabad. It is a unique twist on traditional Indian biryani, and is a combination of spices, rice, yogurt, and either chicken or lamb.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt_template.format(reco_type='dish', location='Hyderabad')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The most popular movie associated with Hyderabad is Baahubali: The Beginning, an Indian epic fantasy film.\n",
      "\n",
      "\n",
      "\"Pearl of the Deccan\".\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt_template.format(reco_type='movie', location='Hyderabad')))\n",
    "print(llm(prompt_template.format(reco_type='place to visit', location='Hyderabad')))\n",
    "print(llm(prompt_template.format(reco_type='phrase', location='Hyderabad')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the schema\n",
    "\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"dish_name\", description=\"This is the name of the dish\"),\n",
    "    ResponseSchema(name=\"dish_description\", description=\"This is a short description of the dish\")\n",
    "]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"dish_name\": string  // This is the name of the dish\n",
      "\t\"dish_description\": string  // This is a short description of the dish\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 2. Generates output format instructions which can be included in your prompt.\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL PROMPT:\n",
      "\n",
      "You will be given a location by the user.\n",
      "Please provide a dish name and a short description of the dish associated with the location.\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"dish_name\": string  // This is the name of the dish\n",
      "\t\"dish_description\": string  // This is a short description of the dish\n",
      "}\n",
      "```\n",
      "\n",
      "% LOCATION:\n",
      "Pune\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Incorporate into final prompt.\n",
    "\n",
    "template = '''\n",
    "You will be given a location by the user.\n",
    "Please provide a dish name and a short description of the dish associated with the location.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% LOCATION:\n",
    "{location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "'''\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format(location=\"Pune\")\n",
    "print('FINAL PROMPT:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"dish_name\": \"Pav Bhaji\",\n",
      "\t\"dish_description\": \"Pav Bhaji is a popular fast food dish originating from the Indian city of Mumbai. It consists of a spicy potato and vegetable curry served with buttered rolls (pav) and a variety of traditional accompaniments.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 4. LLM now outputs an easily parsable response.\n",
    "\n",
    "response = llm(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pav Bhaji'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Output parser can extract named fields in the schema.\n",
    "\n",
    "output_parser.parse(response)['dish_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Create chains (workflows) by running multiple components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain component 1: Get dish for location.\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "template1 = \"\"\"You will be given a location from the user. Please provide a dish associated with the location.\n",
    "\n",
    "% LOCATION\n",
    "{location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template1 = PromptTemplate(input_variables=[\"location\"], template=template1)\n",
    "\n",
    "# LLMChain is a simple chain which takes a prompt template, formats with user input and returns LLM output.\n",
    "location_dish_chain = LLMChain(llm=llm, prompt=prompt_template1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain component 2: Get recipe for a dish.\n",
    "\n",
    "template2 = \"\"\"Given the name of a dish, give a short recipe for making it at home.\n",
    "\n",
    "% DISH\n",
    "{dish}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template2 = PromptTemplate(input_variables=[\"dish\"], template=template2)\n",
    "\n",
    "recipe_chain = LLMChain(llm=llm, prompt=prompt_template2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mPav Bhaji\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "Ingredients: \n",
      "- 2 potatoes, diced\n",
      "- 2 cups cauliflower, diced \n",
      "- 1 onion, diced \n",
      "- 1 green bell pepper, diced \n",
      "- 2 tablespoons vegetable oil \n",
      "- 2 teaspoons cumin seeds \n",
      "- 2 teaspoons mustard seeds \n",
      "- 1 teaspoon turmeric powder \n",
      "- 1 teaspoon garam masala \n",
      "- 2 tablespoons tomato paste \n",
      "- 1 cup cooked peas \n",
      "- Salt, to taste \n",
      "- 2 tablespoons butter \n",
      "- 1 cup cooked sweet corn \n",
      "- 8-10 pav buns \n",
      "- Chopped cilantro\n",
      "\n",
      "Instructions: \n",
      "1. Heat oil in a large pan over medium heat. Add cumin and mustard seeds and let it splutter. \n",
      "2. Add the onions and sauté until they are golden brown. \n",
      "3. Add the potatoes, cauliflower, green bell pepper and sauté for another 2-3 minutes. \n",
      "4. Add the turmeric powder, garam masala and tomato paste and mix well. \n",
      "5. Add the cooked peas, salt and about ½ cup of water to cover all the vegetables and cover it with a lid. Cook until the vegetables are cooked through. \n",
      "6. Add the butter and sweet corn and mix everything together. Cook for another 2-3 minutes. \n",
      "7. Take off heat and mash the cooked vegetables lightly with a potato masher. \n",
      "8. Toast the pav buns on a hot pan and serve the pav bhaji with some chopped cilantro, and butter. Enjoy!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[location_dish_chain, recipe_chain],\n",
    "    verbose=True)\n",
    "chain_output = overall_chain.run(\"Pune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain output will return the response for the final component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingredients: \n",
      "- 2 potatoes, diced\n",
      "- 2 cups cauliflower, diced \n",
      "- 1 onion, diced \n",
      "- 1 green bell pepper, diced \n",
      "- 2 tablespoons vegetable oil \n",
      "- 2 teaspoons cumin seeds \n",
      "- 2 teaspoons mustard seeds \n",
      "- 1 teaspoon turmeric powder \n",
      "- 1 teaspoon garam masala \n",
      "- 2 tablespoons tomato paste \n",
      "- 1 cup cooked peas \n",
      "- Salt, to taste \n",
      "- 2 tablespoons butter \n",
      "- 1 cup cooked sweet corn \n",
      "- 8-10 pav buns \n",
      "- Chopped cilantro\n",
      "\n",
      "Instructions: \n",
      "1. Heat oil in a large pan over medium heat. Add cumin and mustard seeds and let it splutter. \n",
      "2. Add the onions and sauté until they are golden brown. \n",
      "3. Add the potatoes, cauliflower, green bell pepper and sauté for another 2-3 minutes. \n",
      "4. Add the turmeric powder, garam masala and tomato paste and mix well. \n",
      "5. Add the cooked peas, salt and about ½ cup of water to cover all the vegetables and cover it with a lid. Cook until the vegetables are cooked through. \n",
      "6. Add the butter and sweet corn and mix everything together. Cook for another 2-3 minutes. \n",
      "7. Take off heat and mash the cooked vegetables lightly with a potato masher. \n",
      "8. Toast the pav buns on a hot pan and serve the pav bhaji with some chopped cilantro, and butter. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "print(chain_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Q&A workflow using Document loaders, Text splitters, RetrievalQA\n",
    "* The code below uses open AI embeddings and chromadb (pip install chromadb, VC++ build tools needed), but langchain supports many other [vector stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html). Feel free to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Merchant of Venice, by William Shakespeare\n",
      "Document length in characters: 154211\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "# Load a long document.\n",
    "loader = GutenbergLoader('https://www.gutenberg.org/cache/epub/1515/pg1515.txt')\n",
    "doc = loader.load()\n",
    "\n",
    "print(doc[0].page_content[:80].rstrip())\n",
    "print('Document length in characters:', len(doc[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# We need to split long documents into smaller chunks to stay under the LLM token limit.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(doc[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE PRINCE OF MOROCCO, suitor to Portia\\r\\n\\n\\nTHE PRINCE OF ARRAGON, suitor to Portia\\r\\n\\n\\nANTONIO, a merchant of Venice\\r\\n\\n\\nBASSANIO, his friend, suitor to Portia\\r\\n\\n\\nGRATIANO, friend to Antonio and Bassanio\\r\\n\\n\\nSOLANIO, friend to Antonio and Bassanio\\r\\n\\n\\nSALARINO, friend to Antonio and Bassanio\\r\\n\\n\\nLORENZO, in love with Jessica\\r\\n\\n\\nSHYLOCK, a rich Jew\\r\\n\\n\\nTUBAL, a Jew, his friend\\r\\n\\n\\nLAUNCELET GOBBO, a clown, servant to Shylock\\r\\n\\n\\nOLD GOBBO, father to Launcelet\\r\\n\\n\\nLEONARDO, servant to Bassanio\\r\\n\\n\\nBALTHAZAR, servant to Portia\\r\\n\\n\\nSTEPHANO, servant to Portia\\r\\n\\n\\nSALERIO, a messenger from Venice\\r\\n\\n\\n\\r\\n\\n\\nPORTIA, a rich heiress\\r\\n\\n\\nNERISSA, her waiting-woman\\r\\n\\n\\nJESSICA, daughter to Shylock\\r\\n\\n\\n\\r\\n\\n\\nMagnificoes of Venice, Officers of the Court of Justice, a Gaoler,\\r\\n\\n\\nServants and other Attendants\\r\\n\\n\\n\\r\\n\\n\\nSCENE: Partly at Venice, and partly at Belmont, the seat of Portia on\\r\\n\\n\\nthe Continent\\r\\n\\n\\n\\r\\n\\n\\n\\r\\n\\n\\nACT I\\r\\n\\n\\n\\r\\n\\n\\nSCENE I. Venice. A street.\\r\\n\\n\\n\\r\\n\\n\\n Enter Antonio, Salarino and Solanio.\\r\\n\\n\\n\\r\\n\\n\\nANTONIO.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for each passage for retrieval.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": f\"{i}\"} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': ' Bassanio believes Gratiano suffers from a character flaw of being too wild, rude, and bold of voice.\\n',\n",
       " 'sources': '40, 103'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "retriever = docsearch.as_retriever(search_type='similarity', search_kwargs={'k':2})\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type=\"stuff\", retriever=retriever)\n",
    "response = chain({\"question\": \"From what character flaw does Bassanio believe Gratiano suffers?\"}, return_only_outputs=True)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%ANSWER%\n",
      "\n",
      " Bassanio believes Gratiano suffers from a character flaw of being too wild, rude, and bold of voice.\n",
      "\n",
      "%SOURCE%\n",
      "\n",
      "You have obtain’d it.\r\n",
      "\r\n",
      "GRATIANO.\r\n",
      "You must not deny me, I must go with you to Belmont.\r\n",
      "\r\n",
      "BASSANIO.\r\n",
      "Why, then you must. But hear thee, Gratiano,\r\n",
      "Thou art too wild, too rude, and bold of voice,\r\n",
      "Parts that become thee happily enough,\r\n",
      "And in such eyes as ours appear not faults;\r\n",
      "But where thou art not known, why there they show\r\n",
      "Something too liberal. Pray thee, take pain\r\n",
      "To allay with some cold drops of modesty\r\n",
      "Thy skipping spirit, lest through thy wild behaviour\r\n",
      "I be misconst’red in the place I go to,\r\n",
      "And lose my hopes.\r\n",
      "\r\n",
      "GRATIANO.\r\n",
      "Signior Bassanio, hear me.\r\n",
      "If I do not put on a sober habit,\r\n",
      "Talk with respect, and swear but now and then,\r\n",
      "Wear prayer-books in my pocket, look demurely,\r\n",
      "Nay more, while grace is saying, hood mine eyes\r\n",
      "Thus with my hat, and sigh, and say “amen”;\r\n",
      "Use all the observance of civility\r\n",
      "Like one well studied in a sad ostent\r\n",
      "To please his grandam, never trust me more.\r\n",
      "\r\n",
      "BASSANIO.\n"
     ]
    }
   ],
   "source": [
    "def print_answer_with_source(response):\n",
    "    print('%ANSWER%\\n\\n' + response['answer'])\n",
    "    source_ids = [int(x) for x in response['sources'].split(', ')]\n",
    "    print('%SOURCE%\\n\\n' + texts[source_ids[0]].replace('\\n\\n', '\\n').replace('\\n\\n', '\\n'))\n",
    "\n",
    "print_answer_with_source(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents\n",
    "* Some applications will require not just a predetermined chain of calls to LLMs/other tools, but **potentially an unknown chain that depends on the user’s input**. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n",
    "\n",
    "Tools\n",
    "* Langchain has wrappers for many tools - see the full list [here](https://python.langchain.com/en/latest/modules/agents/tools.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import BingSearchAPIWrapper, WikipediaAPIWrapper, PythonREPL\n",
    "\n",
    "# Bing API usage requires the BING_SUBSCRIPTION_KEY and BING_SEARCH_URL environment variables to be present.\n",
    "bingapi = BingSearchAPIWrapper(k=1)\n",
    "wikipedia = WikipediaAPIWrapper(top_k_results=1)\n",
    "python_repl = PythonREPL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"BingSearch\",\n",
    "        func=bingapi.run,\n",
    "        description=\"Used to search the web with a query\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, historical events, or other subjects. Input should be a search query.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"PythonREPL\",\n",
    "        func=python_repl.run,\n",
    "        description=\"Used to run python code for performing complex calculations\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm = OpenAI(temperature=0, model_name=\"text-davinci-003\")\n",
    "react = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out when the Matrix was released and then calculate the difference in time.\n",
      "Action: BingSearch\n",
      "Action Input: \"When was the Matrix released\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe <b>Matrix</b> sold more than 107,000 DVD copies in just two weeks, breaking Armageddon &#39; s record for becoming the country&#39;s best-selling DVD title. The Ultimate <b>Matrix</b> Collection was <b>released</b> on HD DVD on May 22, 2007 and on Blu-ray on October 14, 2008.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find the exact date the Matrix was released.\n",
      "Action: Wikipedia\n",
      "Action Input: \"The Matrix\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: The Matrix\n",
      "Summary: The Matrix is a 1999 science fiction action film written and directed by the Wachowskis. It is the first installment in the Matrix film series, starring Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Hugo Weaving, and Joe Pantoliano, and depicts a dystopian future in which humanity is unknowingly trapped inside the Matrix, a simulated reality that intelligent machines have created to distract humans while using their bodies as an energy source. When computer programmer Thomas Anderson, under the hacker alias \"Neo\", uncovers the truth, he joins a rebellion against the machines along with other people who have been freed from the Matrix.\n",
      "The Matrix is an example of the cyberpunk subgenre of science fiction. The Wachowskis' approach to action scenes was influenced by Japanese animation and martial arts films, and the film's use of fight choreographers and wire fu techniques from Hong Kong action cinema influenced subsequent Hollywood action film productions. The film popularized terms such as red pill, and introduced a visual effect known as \"bullet time\", in which the heightened perception of certain characters is represented by allowing the action within a shot to progress in slow-motion while the camera appears to move through the scene at normal speed, allowing the sped-up movements of certain characters to be perceived normally.\n",
      "The Matrix opened in theaters in the United States on March 31, 1999, to widespread acclaim from critics, who praised its innovative visual effects, action sequences, cinematography and entertainment value, and was a massive success at the box office, grossing over $460 million on a $63 million budget, becoming the highest-grossing Warner Bros. film of 1999 and the fourth highest-grossing film of that year. At the 72nd Academy Awards, the film won all four categories it was nominated for, Best Visual Effects, Best Film Editing, Best Sound, and Best Sound Editing. The film was also the recipient of numerous other accolades, including Best Sound and Best Special Visual Effects at the 53rd British Academy Film Awards, and the Wachowskis were awarded Best Director and Best Science Fiction Film at the 26th Saturn Awards. The film is considered to be among the greatest science fiction films of all time, and in 2012, the film was selected for preservation in the United States National Film Registry by the Library of Congress for being \"culturally, historically, and aesthetically significant.\"The film's success led to two feature film sequels being released in 2003, The Matrix Reloaded and The Matrix Revolutions, which were also written and directed by the Wachowskis. The Matrix franchise was further expanded through the production of comic books, video games and an animated anthology film, The Animatrix, with which the Wachowskis were heavily involved. The franchise has also inspired books and theories expanding on some of the religious and philosophical ideas alluded to in the films. A fourth film, titled The Matrix Resurrections, was released on December 22, 2021.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the exact date the Matrix was released.\n",
      "Action: PythonREPL\n",
      "Action Input: import datetime; datetime.datetime.now() - datetime.datetime(1999, 3, 31)\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the number of minutes since the Matrix was released.\n",
      "Final Answer: The Matrix was released on March 31, 1999, which means it has been approximately 6,822,400 minutes since its release.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Matrix was released on March 31, 1999, which means it has been approximately 6,822,400 minutes since its release.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react.run(\"How many minutes has it been since the Matrix was released?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
