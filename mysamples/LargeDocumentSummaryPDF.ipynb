{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from dotenv import load_dotenv\n",
    "# from langchain.document_loaders import JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"DEFAULT_EMBED_BATCH_SIZE\"] = \"10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(engine=\"davinci\",temperature=0)\n",
    "text_splitter = CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data/LargeLanguageModelsforUserInterestJourneys.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import JSONLoader\n",
    "# loader = JSONLoader(\"data/MedicalRecords.json\", jq_schema='.key[].text')\n",
    "# pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: instruction-tuned counterpart introduced by Wei et al.[ 53]. This pre-trained model was fine-tuned on datasets where\n",
      "each example is prefixed with some combination of instructions and/or few-shot exemplars, and was shown in [ 13] to\n",
      "Manuscript submitted to ACM\n",
      "15: aim at short sentences describing different user interest journeys. Also, while [ 37] focuses on scrutable profiles, with\n",
      "the aim of recommendation, we focus on investigating the degree to which our personalized clustering approach\n",
      "coupled with prompting LLMs can provide deeper user understanding. Perhaps the most important difference is that,\n",
      "to our knowledge, our work is the first-ever experimental study of aligning LLMs to reason through user journeys.\n",
      "Furthermore, our work differs significantly from [ 3] where the authors showed how a restricted representation of users\n",
      "as weighted pairs of tag interaction can be verbalized as NL statements using templates. Our models rely on free-form\n",
      "natural language, rather than templates to output interest names.\n",
      "Manuscript submitted to ACM\n"
     ]
    }
   ],
   "source": [
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings(chunk_size=1))\n",
    "docs = faiss_index.similarity_search(\"what is GPT-4?\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"gpt-35-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm=ChatOpenAI(temperature=0.7, engine=\"gpt-4\")\n",
    "# llm=ChatOpenAI(temperature=0.7, engine=\"gpt-4\", max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intermediate_steps': ['This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, and summarize these journeys using LLMs with techniques like few-shot prompting, prompt-tuning, and fine-tuning. The results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The study also sheds light on the factors impacting the quality of the generated journey names and uncovers the taxonomy of real users\\' journeys on recommendation platforms.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses. The results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses. The results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses. The results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys. The authors propose an infinite concept personalized clustering algorithm and emphasize the importance of granularity, nuance, and safety in generating journey names, ultimately aiming to create more personalized and meaningful user experiences on recommendation platforms.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses. The results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys. The authors emphasize the importance of granularity, nuance, and safety in generating journey names, ultimately aiming to create more personalized and meaningful user experiences on recommendation platforms.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe authors emphasize the importance of granularity, nuance, and safety in generating journey names, ultimately aiming to create more personalized and meaningful user experiences on recommendation platforms. They explore data-efficient techniques like few-shot prompting and prompt-tuning, as well as data-rich end-to-end fine-tuning for aligning LLMs with user journeys. These methods are tested with different volumes of data, and the ideal dataset for this task would involve users annotating and describing their interest journeys in a nuanced and interesting way.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nTo generate journey names with granularity, nuance, and safety, the authors explore data-efficient techniques like few-shot prompting and prompt-tuning, as well as data-rich end-to-end fine-tuning for aligning LLMs with user journeys. They utilize three different data sources: user interviews, user collections, and expert-curated collections, to create a more personalized and meaningful user experience on recommendation platforms. The researchers test these methods with varying data volumes, aiming to achieve an ideal dataset that involves users annotating and describing their interest journeys in a nuanced and interesting way.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nTo generate journey names with granularity, nuance, and safety, the authors explore data-efficient techniques like few-shot prompting and prompt-tuning, as well as data-rich end-to-end fine-tuning for aligning LLMs with user journeys. They utilize three different data sources: user interviews, user collections, and expert-curated collections, to create a more personalized and meaningful user experience on recommendation platforms. The researchers test these methods with varying data volumes, aiming to achieve an ideal dataset that involves users annotating and describing their interest journeys in a nuanced and interesting way. The ICPC algorithm, with a similarity threshold of 0.1, demonstrates its effectiveness in generating coherent journeys of the right granularity when compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThrough a comparison of journey extraction methods, the ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. The algorithm manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThrough a comparison of journey extraction methods, the ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. The algorithm manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys. Testing different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys. Testing different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability. The effect of model size and architecture is also explored, revealing that LaMDA models achieve the best performance in certain conditions, while instruction-tuned models show potential for better few-shot prompting. Finally, the research emphasizes the importance of high-quality prompt tuning data and the usefulness of item titles as the most informative feature for constructing prompts.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys. \\n\\nTesting different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability. The effect of model size and architecture is also explored, revealing that LaMDA models achieve the best performance in certain conditions, while instruction-tuned models show potential for better few-shot prompting. Further analysis shows that expertly curated collection names provide higher quality data for learning the prompt compared to user-provided names, suggesting the potential of mixed methods for enabling LLMs to accurately describe user interests. The study also finds that including more journey items results in better name generation performance. Finally, the research emphasizes the importance of high-quality prompt tuning data and the usefulness of item titles as the most informative feature for constructing prompts.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys. \\n\\nTesting different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability. The effect of model size and architecture is also explored, revealing that LaMDA models achieve the best performance in certain conditions, while instruction-tuned models show potential for better few-shot prompting. Further analysis shows that expertly curated collection names provide higher quality data for learning the prompt compared to user-provided names, suggesting the potential of mixed methods for enabling LLMs to accurately describe user interests. The study also finds that including more journey items results in better name generation performance. Finally, the research emphasizes the importance of high-quality prompt tuning data and the usefulness of item titles as the most informative feature for constructing prompts.\\n\\nIn an early analysis, the authors demonstrate that the LLM can generate significantly better journey names when it is only given the journey-specific history, further validating the need for their journey extraction component in the journey service. Additionally, they find that journey-aware recommendations significantly improve the user experience, with an average of 1 out of 10 journeys being served under the current recommendation system, compared to 0.253 when the recommender is journey-aware. These results highlight the potential of making recommendations more aware of user interest journeys, ultimately paving the way for a more personalized and engaging user experience on recommendation platforms.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys.\\n\\nTesting different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability. The effect of model size and architecture is also explored, revealing that LaMDA models achieve the best performance in certain conditions, while instruction-tuned models show potential for better few-shot prompting. Further analysis shows that expertly curated collection names provide higher quality data for learning the prompt compared to user-provided names, suggesting the potential of mixed methods for enabling LLMs to accurately describe user interests. The study also finds that including more journey items results in better name generation performance. Finally, the research emphasizes the importance of high-quality prompt tuning data and the usefulness of item titles as the most informative feature for constructing prompts.\\n\\nIn an early analysis, the authors demonstrate that the LLM can generate significantly better journey names when it is only given the journey-specific history, further validating the need for their journey extraction component in the journey service. Additionally, they find that journey-aware recommendations significantly improve the user experience, with an average of 1 out of 10 journeys being served under the current recommendation system, compared to 0.253 when the recommender is journey-aware. These results highlight the potential of making recommendations more aware of user interest journeys, ultimately paving the way for a more personalized and engaging user experience on recommendation platforms. The study contributes to the fields of user modeling, context-aware recommendation, and interpretable/transparent user models, emphasizing the value of journey-aware systems in satisfying users\\' real-life interests and aspirations.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys.\\n\\nTesting different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability. The effect of model size and architecture is also explored, revealing that LaMDA models achieve the best performance in certain conditions, while instruction-tuned models show potential for better few-shot prompting. Further analysis shows that expertly curated collection names provide higher quality data for learning the prompt compared to user-provided names, suggesting the potential of mixed methods for enabling LLMs to accurately describe user interests. The study also finds that including more journey items results in better name generation performance. Finally, the research emphasizes the importance of high-quality prompt tuning data and the usefulness of item titles as the most informative feature for constructing prompts.\\n\\nIn an early analysis, the authors demonstrate that the LLM can generate significantly better journey names when it is only given the journey-specific history, further validating the need for their journey extraction component in the journey service. Additionally, they find that journey-aware recommendations significantly improve the user experience, with an average of 1 out of 10 journeys being served under the current recommendation system, compared to 0.253 when the recommender is journey-aware. These results highlight the potential of making recommendations more aware of user interest journeys, ultimately paving the way for a more personalized and engaging user experience on recommendation platforms. The study contributes to the fields of user modeling, context-aware recommendation, and interpretable/transparent user models, emphasizing the value of journey-aware systems in satisfying users\\' real-life interests and aspirations. The research also demonstrates the untapped potential of LLMs in the domain of recommender systems and presents a novel approach to leveraging LLMs for deeper user understanding and building novel user experiences that offer more user control.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys.\\n\\nTesting different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability. The effect of model size and architecture is also explored, revealing that LaMDA models achieve the best performance in certain conditions, while instruction-tuned models show potential for better few-shot prompting. Further analysis shows that expertly curated collection names provide higher quality data for learning the prompt compared to user-provided names, suggesting the potential of mixed methods for enabling LLMs to accurately describe user interests. The study also finds that including more journey items results in better name generation performance. Finally, the research emphasizes the importance of high-quality prompt tuning data and the usefulness of item titles as the most informative feature for constructing prompts.\\n\\nIn an early analysis, the authors demonstrate that the LLM can generate significantly better journey names when it is only given the journey-specific history, further validating the need for their journey extraction component in the journey service. Additionally, they find that journey-aware recommendations significantly improve the user experience, with an average of 1 out of 10 journeys being served under the current recommendation system, compared to 0.253 when the recommender is journey-aware. These results highlight the potential of making recommendations more aware of user interest journeys, ultimately paving the way for a more personalized and engaging user experience on recommendation platforms. The study contributes to the fields of user modeling, context-aware recommendation, and interpretable/transparent user models, emphasizing the value of journey-aware systems in satisfying users\\' real-life interests and aspirations. The research also demonstrates the untapped potential of LLMs in the domain of recommender systems and presents a novel approach to leveraging LLMs for deeper user understanding and building novel user experiences that offer more user control.',\n",
       "  'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys.\\n\\nTesting different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability. The effect of model size and architecture is also explored, revealing that LaMDA models achieve the best performance in certain conditions, while instruction-tuned models show potential for better few-shot prompting. Further analysis shows that expertly curated collection names provide higher quality data for learning the prompt compared to user-provided names, suggesting the potential of mixed methods for enabling LLMs to accurately describe user interests. The study also finds that including more journey items results in better name generation performance. Finally, the research emphasizes the importance of high-quality prompt tuning data and the usefulness of item titles as the most informative feature for constructing prompts.\\n\\nIn an early analysis, the authors demonstrate that the LLM can generate significantly better journey names when it is only given the journey-specific history, further validating the need for their journey extraction component in the journey service. Additionally, they find that journey-aware recommendations significantly improve the user experience, with an average of 1 out of 10 journeys being served under the current recommendation system, compared to 0.253 when the recommender is journey-aware. These results highlight the potential of making recommendations more aware of user interest journeys, ultimately paving the way for a more personalized and engaging user experience on recommendation platforms. The study contributes to the fields of user modeling, context-aware recommendation, and interpretable/transparent user models, emphasizing the value of journey-aware systems in satisfying users\\' real-life interests and aspirations. The research also demonstrates the untapped potential of LLMs in the domain of recommender systems and presents a novel approach to leveraging LLMs for deeper user understanding and building novel user experiences that offer more user control.'],\n",
       " 'output_text': 'This paper explores the potential of large language models (LLMs) in improving personalized user experiences on recommendation platforms. The authors introduce a framework for personalized extraction of \"interest journeys,\" or persistent user interests, needs, and goals, by leveraging personalized clustering from user interaction logs. They propose an infinite concept personalized clustering algorithm (ICPC) that effectively performs online clustering of a user\\'s interaction history based on the salient terms of the comprising items. LLMs are then used to describe these journeys with interpretable and nuanced names through techniques like few-shot prompting, prompt-tuning, and fine-tuning. The study investigates real-life journeys of users, uncovering a taxonomy of valued interest journeys through clustering survey responses.\\n\\nThe results demonstrate the potential of LLMs in providing deeper, more interpretable, and controllable user understanding, paving the way for new user experiences on recommendation platforms that are journey-aware, assistive, and enable frictionless conversation. The research highlights that people value content related to entertainment, learning, and community engagement, with entertainment and learning new skills being the top valued journeys. Additionally, the findings show that users pursue multiple interest journeys concurrently over long periods of time, rely more on explicit actions to find content relevant to their journeys, and that their journeys are nuanced and evolve in a personalized way. This calls for a more user-centric approach in recommendation systems, offering interpretability and control to users in pursuing their interest journeys.\\n\\nThe ICPC algorithm achieves long coherent journeys and is more robust to trivial coherence as obtained by singleton journey clusters. It manages to easily control journey granularity via the similarity threshold and achieves the highest recall compared to baseline methods using co-occurrence-based clusters and multimodal item similarity-based clusters. By utilizing user interviews, user collections, and expert-curated collections, the authors create a more personalized and meaningful user experience on recommendation platforms, confirming the effectiveness of the ICPC algorithm in generating coherent journeys of the right granularity. The journey naming service can provide accurate, nuanced names describing the user interest journeys, such as \"the art of photography\" and \"orgo kart racing,\" illustrating the practical utility of LLMs in reasoning well through user interest journeys.\\n\\nTesting different prompting techniques, the study finds that prompt-tuning on small high-quality data outperforms few-shot prompting and that fine-tuning offers better in-domain performance, while prompt-tuning has better generalization capability. The effect of model size and architecture is also explored, revealing that LaMDA models achieve the best performance in certain conditions, while instruction-tuned models show potential for better few-shot prompting. Further analysis shows that expertly curated collection names provide higher quality data for learning the prompt compared to user-provided names, suggesting the potential of mixed methods for enabling LLMs to accurately describe user interests. The study also finds that including more journey items results in better name generation performance. Finally, the research emphasizes the importance of high-quality prompt tuning data and the usefulness of item titles as the most informative feature for constructing prompts.\\n\\nIn an early analysis, the authors demonstrate that the LLM can generate significantly better journey names when it is only given the journey-specific history, further validating the need for their journey extraction component in the journey service. Additionally, they find that journey-aware recommendations significantly improve the user experience, with an average of 1 out of 10 journeys being served under the current recommendation system, compared to 0.253 when the recommender is journey-aware. These results highlight the potential of making recommendations more aware of user interest journeys, ultimately paving the way for a more personalized and engaging user experience on recommendation platforms. The study contributes to the fields of user modeling, context-aware recommendation, and interpretable/transparent user models, emphasizing the value of journey-aware systems in satisfying users\\' real-life interests and aspirations. The research also demonstrates the untapped potential of LLMs in the domain of recommender systems and presents a novel approach to leveraging LLMs for deeper user understanding and building novel user experiences that offer more user control.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "# chain = load_summarize_chain(llm, chain_type=\"map_reduce\", return_intermediate_steps=False, map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\", return_intermediate_steps=True)\n",
    "chain({\"input_documents\": docs}, return_only_outputs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
