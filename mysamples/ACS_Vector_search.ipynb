{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Azure Cognitive Search SDK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n",
      "Collecting azure-search-documents==11.4.0a20230509004\n",
      "  Downloading https://pkgs.dev.azure.com/azure-sdk/29ec6040-b234-4e31-b139-33dc4287b756/_packaging/3572dbf9-b5ef-433b-9137-fc4d7768e7cc/pypi/download/azure-search-documents/11.4a20230509004/azure_search_documents-11.4.0a20230509004-py3-none-any.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting azure-core<2.0.0,>=1.24.0 (from azure-search-documents==11.4.0a20230509004)\n",
      "  Downloading https://pkgs.dev.azure.com/azure-sdk/29ec6040-b234-4e31-b139-33dc4287b756/_packaging/3572dbf9-b5ef-433b-9137-fc4d7768e7cc/pypi/download/azure-core/1.27.1/azure_core-1.27.1-py3-none-any.whl (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting azure-common~=1.1 (from azure-search-documents==11.4.0a20230509004)\n",
      "  Downloading https://pkgs.dev.azure.com/azure-sdk/29ec6040-b234-4e31-b139-33dc4287b756/_packaging/3572dbf9-b5ef-433b-9137-fc4d7768e7cc/pypi/download/azure-common/1.1.28/azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting isodate>=0.6.0 (from azure-search-documents==11.4.0a20230509004)\n",
      "  Downloading https://pkgs.dev.azure.com/azure-sdk/29ec6040-b234-4e31-b139-33dc4287b756/_packaging/3572dbf9-b5ef-433b-9137-fc4d7768e7cc/pypi/download/isodate/0.6.1/isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in ./.venv/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0a20230509004) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in ./.venv/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0a20230509004) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.venv/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0a20230509004) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0a20230509004) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0a20230509004) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0a20230509004) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.24.0->azure-search-documents==11.4.0a20230509004) (2023.5.7)\n",
      "Installing collected packages: azure-common, isodate, azure-core, azure-search-documents\n",
      "Successfully installed azure-common-1.1.28 azure-core-1.27.1 azure-search-documents-11.4.0a20230509004 isodate-0.6.1\n",
      "Collecting azure-identity\n",
      "  Downloading azure_identity-1.13.0-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: azure-core<2.0.0,>=1.11.0 in ./.venv/lib/python3.10/site-packages (from azure-identity) (1.27.1)\n",
      "Collecting cryptography>=2.5 (from azure-identity)\n",
      "  Downloading cryptography-41.0.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting msal<2.0.0,>=1.20.0 (from azure-identity)\n",
      "  Downloading msal-1.22.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msal-extensions<2.0.0,>=0.3.0 (from azure-identity)\n",
      "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from azure-identity) (1.16.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in ./.venv/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.11.0->azure-identity) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./.venv/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.11.0->azure-identity) (4.6.3)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity)\n",
      "  Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyJWT[crypto]<3,>=1.0.0 (from msal<2.0.0,>=1.20.0->azure-identity)\n",
      "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting portalocker<3,>=1.0 (from msal-extensions<2.0.0,>=0.3.0->azure-identity)\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=2.5->azure-identity)\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (2023.5.7)\n",
      "Installing collected packages: PyJWT, pycparser, portalocker, cffi, cryptography, msal, msal-extensions, azure-identity\n",
      "Successfully installed PyJWT-2.7.0 azure-identity-1.13.0 cffi-1.15.1 cryptography-41.0.1 msal-1.22.0 msal-extensions-1.0.0 portalocker-2.7.0 pycparser-2.21\n"
     ]
    }
   ],
   "source": [
    "!pip install --index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ azure-search-documents==11.4.0a20230509004\n",
    "!pip install azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure OpenAI and vector store settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = os.getenv('OPENAI_API_TYPE')\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_version = os.getenv('OPENAI_API_VERSION')\n",
    "# #openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "model: str = \"text-embedding-ada-002\"\n",
    "vector_store_address = os.getenv('AZURE_SEARCH_ENDPOINT')\n",
    "vector_store_password = os.getenv('AZURE_SEARCH_ADMIN_KEY')\n",
    "index_name: str = \"langchain-vector-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create embeddings and vector store instances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings(model=model, chunk_size=1)\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data/SparkOfGPT-4.pdf\")\n",
    "# pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert text and embeddings into vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms. Operation under Azure OpenAI API version 2023-05-15 have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 2 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MDkzN2Q5ODYtMzA2NS00ZmNiLTljOTAtNWViNzhhYzAzYmNh',\n",
       " 'NjYyY2M4NmQtMmM4Mi00MTk1LWI5ODEtN2E2ZjQ2MzA1ODcx',\n",
       " 'ZWIxNzk5Y2UtODgwYy00OWI2LWFjODEtODQwMTIyNDc1MTdh',\n",
       " 'NmVmODhiYzYtNDliZS00ZjlmLTk1ODUtZWRlNGM4OWZkMGZi',\n",
       " 'OGM4MjU5OGUtYmIyZi00NTNjLWE4NjMtZjE3NWY4YTljZjY4',\n",
       " 'NGI3MmM0ZDEtZDNjNi00OThmLTliNWQtYWJjMTNiMTUxZjEx',\n",
       " 'MDM5MTk1NmQtOGNhZi00ODk2LTk3YjYtMGIzM2Y2MmZmZTNj',\n",
       " 'YTk2OGY3ZWQtYzY2NC00MWZmLWE2YWMtZGZjZjAwMmJkZjQ2',\n",
       " 'OGU5ZDcxMzUtMjU1MC00ZDNjLWI2YjQtZWNjY2QxMTdhNTM2',\n",
       " 'ZjEzMmQ0NWUtNzc1ZS00MjA4LWE5ZDctZGNkNWYyY2QwY2Ri',\n",
       " 'MGQ4MzNiYWQtMDM4Ni00M2QxLWFjYjMtNGUyZjI5YzcyOTNk',\n",
       " 'M2Q5NWFiYWEtYzM5My00OTk5LWJmMzItYWU1MDQzMjQwOTQy',\n",
       " 'MTJiOTk0YTctNTNiOS00NzUwLTg5YWItM2NhYmY3NDgzNDkz',\n",
       " 'NjU3OWY4ODAtMTA1Zi00MTBiLWI0ZjAtMWY4NTY5YmI4ZjNk',\n",
       " 'MjIyOGViZDQtOTYyOC00YzNhLWJmZTMtMTA5YjQ1NGUzMTk3',\n",
       " 'OTAzYzg3NjktNzRjYi00NmVhLWIyYmQtZmVmMmJmYzA0YzVm',\n",
       " 'YmNmNjg0MDUtM2U0Ny00MmMzLWI1OGQtYjAyNGExN2UzNGJh',\n",
       " 'ZDVlNzkxMjMtYjI2ZS00Y2NkLTkwMDItZmI0MTI3NzkxNzk5',\n",
       " 'NDc3YTUwMWYtZjA3Yi00YWZmLWI5OGMtNWE2ZmE1NDY0NDFj',\n",
       " 'N2VhYTAzZTUtZDUxOS00MzIxLWJjYjctMjMzMjEyYzA3YjU3',\n",
       " 'NmY2NTg1ZWItMWQ1OC00Y2VjLThiNDUtODAwZTk0MjFhYzg4',\n",
       " 'MzVkYTRmNzgtYzRmZC00OTU3LWI0ZWItZjAwNzMxMjEwZWY3',\n",
       " 'NmVlYTg3OGItMzkzOS00NmU3LWIyYjMtM2RlYWJhMjY4Njc5',\n",
       " 'NmQ0MDcyNjQtMGViOC00NmM3LWJmMWEtMWU3NmYyMTVlZWEw',\n",
       " 'YmI0YTBhZDktMzIwYy00N2MwLWJlODYtYTVmOTBlYjE2NDVj',\n",
       " 'YTU5MDNlNTktNTA3Zi00MTlmLWE0YzMtYjdhOGZjNDg1MGQy',\n",
       " 'NjE2YTM4MDMtMjFhMS00MWUzLThkNzktMThlYTQ1N2U2MWMz',\n",
       " 'ZGM3ZjliMDUtMjRmOS00ZmI3LWFhMDItMGVmMjVjZDJlMGVj',\n",
       " 'MjU0ZTIzMjgtM2RmZS00MWY2LWEyYjQtZWViZWFlNzcxYjUx',\n",
       " 'MDRiZTRiYTQtNDAwMS00YWQyLTg3NTktMGZiYzFjMWFiMGY0',\n",
       " 'MTZkNGE3OTItMzc0My00ODBjLWE0OTEtZGM5OGY5YTVkODM1',\n",
       " 'YWU4ZWFmODctMTFkNi00MzU0LTllZmUtZjFlOTdiOTIwODE0',\n",
       " 'M2Q0N2ZhOGYtOTI1NC00M2RiLWJjOGYtODhkNjI3OGE5YzNj',\n",
       " 'MDNkMzIyNWItMzM3OC00NzM5LTgyMjEtYTMxMDk2YTExOTA5',\n",
       " 'M2RlYzhjM2YtYzYzMS00NTkyLTg0ODctODIyYjRhYTkzMWFm',\n",
       " 'ZGRmYWIzMzAtOGM2Yi00NjgwLWI4MjAtMjY3YzNlZTA1YjEx',\n",
       " 'ZTc1OTYwOGYtODhiMi00MmU5LWI1MGUtOTMyMWQ0MzZmZWVh',\n",
       " 'MzkzNTA0NDktNzIyYS00MzhiLWEwOGEtMjljMGQ3NmVlMmJj',\n",
       " 'ODY1MWRiMjktYTVmMi00ZTRlLTk3MTMtNWVhNGI2MjQzZGZi',\n",
       " 'ZjJiNGIyNmUtNTNjMy00ZDA1LThkOGQtODk1YzRmY2U2YTc3',\n",
       " 'ZDViOWMxYzYtODBkZC00OTFlLTg4OGYtNTI5ZDgwZDdmZmUw',\n",
       " 'ZTFkMDljNjQtMzg1NS00YTI4LWJhNzItMTAxOGQ3NTNkN2U3',\n",
       " 'MzczZjJlNDYtZGY4NC00MzA1LWExZDItNTkxNmMxMDEwZTc1',\n",
       " 'MjczOTcwYTYtZmIwNy00NmI3LTljNGYtMzViZGU2ZjdjZDAz',\n",
       " 'ZmE3ZTY1ODEtNTNmZC00NWQwLWI0ZGMtNDBlOTQyN2NkOWIw',\n",
       " 'ODJkMTQwZmItZGIzNi00YjAwLTgyMjgtY2JlNmQwOTI0NTli',\n",
       " 'YWNiYjhlNGEtODQwNi00ZmU1LTk5NjMtMWU3NWEwYjA3MmZi',\n",
       " 'Nzc0MzRjYTItOWRlYy00ZGFmLTg3NGEtY2RmZjQ2Zjg5YzNh',\n",
       " 'ZWU1ZjM5ZjgtZTE4ZC00NTRjLThkYmQtMDFiZGJhYjMxYWMy',\n",
       " 'NTEzYzNhNTctNjE2MC00Yzc0LTk3NjEtOTU5MWJkODU4NDA5',\n",
       " 'ZTFjNzExYzEtNjJmMi00MjlmLTk4NDAtZjBiNGRkNTk1ZmRl',\n",
       " 'ZDkzMTE5ZTMtNzUzZi00OTc0LTgwODgtYTk5NzM1YzYxMWU3',\n",
       " 'ODAzYzUwOTEtOTM1ZC00NjAzLThmYzEtZmZmOGFlZTcwYmYy',\n",
       " 'NjA0NGEzYmYtMzlmNS00MTRkLTg4MGItOTEzMTAxZWMxNGFk',\n",
       " 'MzdjYTliMmQtNWVhNi00NzlmLThmNWYtYTA5MWZiNzliNjZl',\n",
       " 'NDRlZWEyZGQtMTY0ZC00ZTVhLWI3YzAtZDBmYjY1YzJlZmY4',\n",
       " 'ODUwNTJhOGMtYTIyYS00NjE5LWJiZjQtNWZhZjM1ODQ1MzI3',\n",
       " 'MjQ4Zjk3MWItNGI0OS00NTM5LThmZjMtZDdlOTE0MDU5MmM0',\n",
       " 'YzVkZDllMGEtMmEzZS00NWE3LTkwMzktNjJlNzU1MDVlNzMz',\n",
       " 'ZDVlMTM1ZDctM2NkMS00NTExLWFmZmItOTIzNDhiMGI4NzAz',\n",
       " 'ZDY1OWEzNmMtNzNjZC00ZmVmLTg0YjEtNTQxNDFhYjkyMDNm',\n",
       " 'YmJiZGJkZDctODg5Mi00ZWNkLWIzNzMtYjhiYmYwYjMxODIw',\n",
       " 'Y2ZlMTBlNzQtZTllMi00NjA1LWI2ZTgtOWZkMmEzM2M0NTNi',\n",
       " 'Njk2OTgzMjYtZDA2Mi00ODljLWI5ODAtY2YxODIzY2MzN2Mx',\n",
       " 'YmM1ODk2ZjgtNzFkZS00MWIxLWJmNDktYzFhYTk2ODY2Zjc5',\n",
       " 'ZDNkNmU0YzktOWZlYi00NGMzLWIwNzItMjg2ZGM3MzZhODBk',\n",
       " 'NjliMjAxNWUtNDhhNy00YjJhLThhYzUtY2I3YWUxNWM5ZTZk',\n",
       " 'MTAzYTAzY2EtMjMyOS00MzU2LWIwNmYtMzU2MjgxNmZjMWNi',\n",
       " 'ODkwNWM1Y2UtMzNhYi00ZTFjLTkxNWYtOTZiM2Q0MTM2NDBi',\n",
       " 'ZTllNTg1YmUtNmYyMi00YmMwLTg1YzAtZDdlMjZhYWYwOWJh',\n",
       " 'ZmQ4MzBjN2UtNjdkMS00OWFkLWE4MGEtZTM0OGI2OGNmZGVl',\n",
       " 'MTYzMDRhMjctZmEyZC00YWMzLThkZDUtZmVlNTY0Y2FhMTgy',\n",
       " 'YjU2Nzc5OWYtMjVmNi00OTc0LWEzNTctYTYxOWIyODUxZjhk',\n",
       " 'ODM3NjNjNDYtZDNhYy00Y2RlLTlmZjItZDU3YmU1ZDgwNzU1',\n",
       " 'NzY5MDA5NjAtOTMyZC00NmI1LThiYTctNjA3M2FmMzgxNWRj',\n",
       " 'ZDgwOTg2NmQtMWM5OS00YTIxLTlhZDAtM2JkYjM0OWQyMWU0',\n",
       " 'ODA4Yjc3YzUtOWM2ZS00MDU3LWFlMWQtZTg3ZDdlNzM2YmMx',\n",
       " 'YjYyZWMzZGYtYTQzOC00MWRkLWE4MjEtMTM1N2VkMThjZDIx',\n",
       " 'MGEwYzU5MGQtOWM5OS00MzE1LWI1YWEtOGI2NWQ5ZDY3ZjM0',\n",
       " 'N2E2M2YwN2YtODIyYy00MzFmLWI1NTgtNzgwNDk3ZThhNDU0',\n",
       " 'NjU0MDMyZmItNDM5OC00ODliLWE4YmYtOGQyY2VjYmNmZTFh',\n",
       " 'NWEzNWI3ZmEtYTdmNy00ZTEwLWFkNjktZjEwNDBhZjNjZmZh',\n",
       " 'Y2QzZDkxZTgtNzFjZC00MGIwLTk5MjItZmEzYzJkOWI3MDg0',\n",
       " 'NTc2OTA3ODUtMDk2MC00ZjdmLWJlY2MtODQ0NWYwY2RmOGI5',\n",
       " 'NmVlNmFkMGItNjQ5ZS00YTQ3LTk5OGEtODQ5NjkyZGI5NTA4',\n",
       " 'N2FmNDU4NmMtN2JkNS00ZDI1LTg2NDItYzg3Y2E0YzE1YmY5',\n",
       " 'NGE0ZDM4NTgtYzYzNy00MjZjLWJhZmEtNjhmNTlhYzgzZDE0',\n",
       " 'Y2VhYzViZjctYzViMy00MDNmLWFiYTEtYmZkYWE1YmNjMTE4',\n",
       " 'OGZhODgxMjItZDA2Yy00NmJmLTljOTctMDllNmU5MTc1ODM3',\n",
       " 'OGZkODVmM2EtZGI5MS00MmU4LTkyZDctNDZiZjVmYTc3MGVk',\n",
       " 'NjU1ODM0MjYtNjJhOC00OTc5LThiZGUtNjNiMDQzZDk0M2Zm',\n",
       " 'MjBiYTdkZGMtOTQ0Zi00MmYwLTg5NTktYjhkMzVhN2JjNTg4',\n",
       " 'OTYzMTA5NjgtNDJlZi00NzMzLWE5YmItZjVmZmU1NDczNGQw',\n",
       " 'MGJiOTFhOWQtN2Y3Yi00M2ZhLWIwOTItZjUwZmE0YWEwMWEz',\n",
       " 'Y2EzZTI3MTgtOTM3My00YWU1LWEyZWItNjdkMTQ0NzYwZDk3',\n",
       " 'OTk4MjkzMzctODU2Yi00ZjNmLTgyNDItMzA2NWRhOTcxMjI0',\n",
       " 'M2MxNWQyZTktZGY3Ny00ZTY5LWE0OGYtMDg3NmE2YTcyNTFi',\n",
       " 'MDc0ZmIzN2MtM2I4NS00NzNiLWEwZjctNDcwZWEzZGRlMDBl',\n",
       " 'ZGU5Y2JjYTItMzYxYi00MWY0LTgwY2QtNzY5MjFlMzA0Mjg4',\n",
       " 'OWU4YjIxZTUtZGM3MC00OTEzLWIxMjgtY2M5MmM5MjhlZDlk',\n",
       " 'MzNiNDk3MjgtZTU5NS00NWIxLTgxNTAtNTgyODAyMTliMjll',\n",
       " 'OWEwMjk1NzktNDczYi00OGU1LTk3YzktOGZmZjg0N2MwMzEy',\n",
       " 'ODllOTAyOWYtN2U1NC00ZDkwLWJlMmItMzFjZTRmZTQ1Mjhh',\n",
       " 'MmNmYzU5MzYtOTUzMC00ODhmLTkzMWUtODEwNTA1MjBjODg1',\n",
       " 'MDliZWNkMTctMTFlMC00Mzc1LTk5NjAtNzQ1NjMwNjAxMDI0',\n",
       " 'Y2MyYWQ2ZDEtYzQ0ZS00MTQ2LTk3OGEtMTY0ZmI1OTE1ZTZl',\n",
       " 'NzQ1NGE4MTAtMDNhOC00ZTZmLTllYzgtZmUxM2Q2MDg5YmVl',\n",
       " 'MGEyNDdhZDgtZTE2Yi00OTM3LTkxNWEtNGMyMjc4OThmYzM2',\n",
       " 'ZTA1ZmJlOWItNDFkMC00NDQ1LWI3ZjAtZjI4Y2Y4NmNlZjEx',\n",
       " 'MjMxYTMwMTUtZGU2Zi00NGVmLWE5MzQtMjQwYTZhZDdiMDYx',\n",
       " 'NzE2YWYyMTgtMWM2OS00ZjgwLTgzY2QtODlmYWFhYjRhM2My',\n",
       " 'YWE0YzFkNGQtNWIwYy00OGRkLWIwMzItNjAzZjNlMzA5NWI5',\n",
       " 'OGM2OGNjMWQtMjgzNy00ZTQ4LWJhZGYtYjk1OWE0YWViZTAw',\n",
       " 'OGIxZWU0MjYtMjJlNy00M2RiLWExNTEtMDYyOGQ0ZDIyZjY3',\n",
       " 'YzQzZTRiZDAtOWExZC00ZWQ1LWEyODktZmU1ZmJlYmM2Yjhi',\n",
       " 'NTZlMThlNzktNDUxMC00M2FhLWI0YWUtY2U1NWNmYjBkZWMw',\n",
       " 'ZDNjZjYzODYtZjI1MS00MDJiLWE4MTUtODZmMTBmOTRhNjdj',\n",
       " 'YzdkYTE1ZmUtNzYyNS00MWQzLWE0NWMtMjQ3ZDMyNTk2MjU5',\n",
       " 'NDQxMzAzYTktMjQxYy00MjdjLWI4YTUtZTQ1YWUzMWJmNzc5',\n",
       " 'MjBiNTExNTctMWQ2NC00ZTM2LWFlNzMtNDI3YzcwZjA0NTVh',\n",
       " 'M2VhZWI2YmMtYzI0NS00ZjA2LTgwNTEtYWM4MGY0NzkyNTAz',\n",
       " 'Mjg5OTk4ZGQtNGZkNC00MDMzLTg1ODEtYzNiYWNjYTgzZGFj',\n",
       " 'NmI3ZmI1NWItZDVkOC00YTE1LWEyZjktODYzNTZjNmIxOGNh',\n",
       " 'NmYzNTU4ZDAtNmUwMC00MTcxLTkxMWItOTZjNjI3MzQ4YmQ0',\n",
       " 'NDhhZjcxMDYtMmU1Ni00NWFiLWFhZTYtOWQ2MGQ4MTg4Mjhk',\n",
       " 'MjIzOThmZDYtNDM3Zi00NzRiLWExOTQtOTBmNTQ3M2RkNjI0',\n",
       " 'YjZkYzZmZDctMjZhMC00NTU4LTk4NTMtNTg4NGJjNzFjN2Uz',\n",
       " 'OGZhMjYyOWEtMTM1MC00YTVkLWI2OTctNWVlYTE1ZjgzMDU1',\n",
       " 'YzlkZjJjZmUtNmY5Ni00YTEwLWJlNTAtMDU0ODI1MzZkN2M3',\n",
       " 'MDA3OWNmZjktMDc3OC00OGI0LTlhZWEtZDM5MWFkYWVmMzEy',\n",
       " 'NmE3YjMzNzUtYTYwNC00ZjFlLWI2ZGUtYjA3NGNhOTZhYjE2',\n",
       " 'NTIxNjgxZGUtNjkzZi00OGVjLWJiZDEtZDE2N2JlYzhiY2Fj',\n",
       " 'YWI3MmFiYWUtNDljOS00MThiLWI5ZmEtMzAzNjhhOGI1ZjJl',\n",
       " 'OGZhYTJmZjAtNmQ4My00YWE3LWFiM2ItYTIxM2I4ODA5MWEz',\n",
       " 'ZTA5NTY0NjAtODJmYS00ZjhhLTljMGQtNmRjN2VhNDMwMjg2',\n",
       " 'OTIyODcwZGUtNDhlMy00ZWNlLTgzMTAtMGQ4NzMzNDFkNjk5',\n",
       " 'NWIzNDVlZjctYjBlMS00NTgyLTkwNGYtMWU1OGZhZmFkOTVl',\n",
       " 'ZWM1ZDkzNzktNjkwZC00MDc3LTg2NWYtY2Y3ZjM5OTdhZTA4',\n",
       " 'YzE4YTQ1OWMtZTgyZC00ZDA4LWJkNjEtMjhhM2ViZmFiN2Vj',\n",
       " 'ODc4NjViNTEtMzlhMC00NTJiLTgwMGUtYWE2Nzg1NGYwOTg2',\n",
       " 'NWRmOGMxZTUtOGMyNy00ZTIxLTliYWEtODk5NjJiMGY3OTc2',\n",
       " 'ZWU2OGQ1NjgtNDQ0YS00YzNjLWJkMmUtZjI0MWQ4ZWM3ZWRh',\n",
       " 'MTIwODY5NTAtZGI0OC00YmVjLTk2N2QtODYxNmMyNjQ0NWM0',\n",
       " 'NmYwZjlhYjEtMTQ4Yi00ODJhLTkxMGYtZDNlZTc5ZmVhNzUz',\n",
       " 'ODdiNjk4MzItODkzNC00M2ZmLTlhNzUtYjk3NzI3MjI2NTZi',\n",
       " 'NzExMGYzMmQtNmM5Yi00NmI0LThkZjUtMzEzM2Y5ZDAxZTZh',\n",
       " 'YjFkMjdkNjQtZDgzYS00NzNjLThlYzItYzQ1ZDE4ZTA2YTA2',\n",
       " 'MDA4NzgzZjMtNTE2NS00NDE2LTkwZGItNDQ4YWFlMmE0MGU0',\n",
       " 'YzQ4NDg0ZWEtMDI3Zi00MDdlLWE4OGUtODkzNjAyOTc0NWMx',\n",
       " 'N2I4NzM2MzYtMzFkMS00ZTQwLTlhZmMtYmVhZmE5NTk0NjJh',\n",
       " 'ZTJhOWQ0OTAtZjRjOS00NjhkLWFjMDMtZmI5MzNmYTdlNDgz',\n",
       " 'NTNhZDRmM2EtYjY1YS00NWRmLWE4OGEtYzFmNjM1NDg5ZjJl',\n",
       " 'NzRlNjY4ZTgtYWYzYy00NWJkLWJjOTgtOTI3ODZkYjY5M2Nl',\n",
       " 'NjM4NWNmM2UtY2JhYi00OWU5LWJmMWMtZTg5YTI5YjcxNWE2',\n",
       " 'OWU3YzMxNzYtZTYwOC00ZjEwLTgzNWQtYWNkNjgzYjAzZmE1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform a vector similarity search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1.5: GPT-4 passes mock technical interviews on LeetCode. GPT-4 could potentially be hired\n",
      "as a software engineer3.\n",
      "preliminary tests (see [Ope23] for much more) on the multiple choice component (majority of the score)\n",
      "of the US Medical Licensing Exam Step 1, 2, and 3 with an accuracy around 80% in each. A similar\n",
      "preliminary test of GPT-4's competency on the Multistate Bar Exam showed an accuracy above 70%.\n",
      "We note that the emergence of human-level abilities in these domains has recently been observed with the\n",
      "latest generation of LLMs, e.g., see [LAD+22, SAT+22] for Google's PaLM on respectively mathematics\n",
      "and medicine, and [BIK22] for GPT-3.5 on in law. Our approach to study GPT-4 is di\u000berent from these\n",
      "works, as we explained previously.\n",
      "3. In Section 5, we test the model's ability to plan as well as to some extent to learn from experience by\n",
      "ipping the table, simulate a game environment), as well as interact\n",
      "with tools. In particular, the fact that GPT-4 can use tools (including itself) will certainly be of immense\n",
      "importance to build real-world applications with GPT-4.\n",
      "4. An important part of our argumentation is that GPT-4 attains human-level performance on many\n",
      "tasks. As such, it is natural to ask how well GPT-4 understands humans themselves. We show several\n",
      "experiments on this question in Section 6, both in terms of understanding humans as well as GPT-4\n",
      "making itself understandable to humans, i.e., addressing the problem of explainability. We note in\n",
      "particular that such tasks require a great deal of common sense , which so far has been a well-known\n",
      "pain point for LLMs [DM15]. In Figure 1.7, we give a \frst example of how much better GPT-4 is at\n",
      "commonsense questions compared to ChatGPT, and provide some further examples in Appendix A.\n",
      "5. Throughout the paper we emphasize limitations whenever we found one, but we also dedicate Section 8\n",
      "to an in-depth analysis of the lack of planning, likely a direct consequence of the autoregressive nature\n",
      "of GPT-4's architecture.\n",
      "6. Finally in Section 9, we discuss the expected societal impact of this early form of AGI, and in Section 10,\n",
      "we share key challenges, directions, and next steps for the \feld.\n",
      "A question that might be lingering on many readers' mind is whether GPT-4 truly understands all these\n",
      "y, without anyhether it just became much better than previous models at improvising on the \n",
      "ip, and that understanding. We hope that after reading this paper the question should almost \n",
      "y improvisation.t wondering how much more there is to true understanding than on-the-\n",
      "Can one reasonably say that a system that passes exams for software engineering candidates (Figure 1.5) is\n",
      "notreally intelligent? Perhaps the only real test of understanding is whether one can produce new knowledge ,\n",
      "such as proving new mathematical theorems, a feat that currently remains out of reach for LLMs.\n",
      "3We test GPT-4 on LeetCode's Interview Assessment platform, which provides simulated coding interviews for software engineer\n",
      "positions at major tech companies. GPT-4 solves all questions from all three rounds of interviews (titled online assessment, phone\n",
      "interview, and on-site interview) using only 10 minutes in total, with 4.5 hour allotted. According to LeetCode, in those three\n",
      "rounds respectively, (the early version of) GPT-4 achieves 8.96/10, 8.69/10, and 10/10 scores and beats 93%, 97%, and 100% of all\n",
      "users (\\score is determined by factors such as the time taken, testcases solved per question, and more\"). See Section 3 for more on\n",
      "GPT-4's coding abilities.\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity search\n",
    "docs = vector_store.similarity_search(\n",
    "    query=\"What are the areas of improvement for GPT-4?\",\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    ")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/SparkOfGPT-4.pdf', 'page': 8}\n",
      "{'source': 'data/SparkOfGPT-4.pdf', 'page': 8}\n",
      "{'source': 'data/SparkOfGPT-4.pdf', 'page': 70}\n",
      "{'source': 'data/SparkOfGPT-4.pdf', 'page': 71}\n",
      "{'source': 'data/SparkOfGPT-4.pdf', 'page': 71}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1.5: GPT-4 passes mock technical interviews on LeetCode. GPT-4 could potentially be hired\n",
      "as a software engineer3.\n",
      "preliminary tests (see [Ope23] for much more) on the multiple choice component (majority of the score)\n",
      "of the US Medical Licensing Exam Step 1, 2, and 3 with an accuracy around 80% in each. A similar\n",
      "preliminary test of GPT-4's competency on the Multistate Bar Exam showed an accuracy above 70%.\n",
      "We note that the emergence of human-level abilities in these domains has recently been observed with the\n",
      "latest generation of LLMs, e.g., see [LAD+22, SAT+22] for Google's PaLM on respectively mathematics\n",
      "and medicine, and [BIK22] for GPT-3.5 on in law. Our approach to study GPT-4 is di\u000berent from these\n",
      "works, as we explained previously.\n",
      "3. In Section 5, we test the model's ability to plan as well as to some extent to learn from experience by\n",
      "ipping the table, simulate a game environment), as well as interact\n",
      "with tools. In particular, the fact that GPT-4 can use tools (including itself) will certainly be of immense\n",
      "importance to build real-world applications with GPT-4.\n",
      "4. An important part of our argumentation is that GPT-4 attains human-level performance on many\n",
      "tasks. As such, it is natural to ask how well GPT-4 understands humans themselves. We show several\n",
      "experiments on this question in Section 6, both in terms of understanding humans as well as GPT-4\n",
      "making itself understandable to humans, i.e., addressing the problem of explainability. We note in\n",
      "particular that such tasks require a great deal of common sense , which so far has been a well-known\n",
      "pain point for LLMs [DM15]. In Figure 1.7, we give a \frst example of how much better GPT-4 is at\n",
      "commonsense questions compared to ChatGPT, and provide some further examples in Appendix A.\n",
      "5. Throughout the paper we emphasize limitations whenever we found one, but we also dedicate Section 8\n",
      "to an in-depth analysis of the lack of planning, likely a direct consequence of the autoregressive nature\n",
      "of GPT-4's architecture.\n",
      "6. Finally in Section 9, we discuss the expected societal impact of this early form of AGI, and in Section 10,\n",
      "we share key challenges, directions, and next steps for the \feld.\n",
      "A question that might be lingering on many readers' mind is whether GPT-4 truly understands all these\n",
      "y, without anyhether it just became much better than previous models at improvising on the \n",
      "ip, and that understanding. We hope that after reading this paper the question should almost \n",
      "y improvisation.t wondering how much more there is to true understanding than on-the-\n",
      "Can one reasonably say that a system that passes exams for software engineering candidates (Figure 1.5) is\n",
      "notreally intelligent? Perhaps the only real test of understanding is whether one can produce new knowledge ,\n",
      "such as proving new mathematical theorems, a feat that currently remains out of reach for LLMs.\n",
      "3We test GPT-4 on LeetCode's Interview Assessment platform, which provides simulated coding interviews for software engineer\n",
      "positions at major tech companies. GPT-4 solves all questions from all three rounds of interviews (titled online assessment, phone\n",
      "interview, and on-site interview) using only 10 minutes in total, with 4.5 hour allotted. According to LeetCode, in those three\n",
      "rounds respectively, (the early version of) GPT-4 achieves 8.96/10, 8.69/10, and 10/10 scores and beats 93%, 97%, and 100% of all\n",
      "users (\\score is determined by factors such as the time taken, testcases solved per question, and more\"). See Section 3 for more on\n",
      "GPT-4's coding abilities.\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Perform a hybrid search\n",
    "docs = vector_store.similarity_search(\n",
    "    query=\"What are the areas of improvement for GPT-4 over GPT-3.5?\",\n",
    "    k=5,\n",
    "    search_type=\"hybrid\",\n",
    ")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Figure 1.3: We queried GPT-4 three times, at roughly equal time intervals over the span of a month\\nwhile the system was being re\\x0cned, with the prompt \\\\Draw a unicorn in TikZ\". We can see a clear\\nevolution in the sophistication of GPT-4\\'s drawings.\\nis backed up by a rich theoretical framework [SSBD14, MRT18]. However, this methodology is not necessarily\\nsuitable for studying GPT-4, for two reasons. First, since we do not have access to the full details of its vast\\ntraining data, we have to assume that it has potentially seen every existing benchmark, or at least some similar\\ndata. For example, it seems like GPT-4 knows the recently proposed BIG-bench [SRR+22] (at least GPT-4\\nknows the canary GUID from BIG-bench). Of course, OpenAI themselves have access to all the training\\ndetails, and thus their report [Ope23] contains a lot of detailed benchmark results. Nevertheless, the second\\nreason for going beyond traditional benchmarks is probably more signi\\x0ccant: One of the key aspects of GPT-\\n4\\'s intelligence is its generality, the ability to seemingly understand and connect any topic, and to perform\\ntasks that go beyond the typical scope of narrow AI systems. Some of GPT-4\\'s most impressive performance\\nare on tasks that do not admit a single solution, such as writing a graphic user interface (GUI) or helping a\\nhuman brainstorm on some work-related problem. Benchmarks for such generative or interactive tasks can\\nbe designed too, but the metric of evaluation becomes a challenge (see e.g., [PSZ+21] for some recent progress\\non this active research area in NLP). We note that criticisms of the standard approach to measure AI systems\\nwere also made in [Cho19], where a new benchmark was proposed to evaluate general intelligence. We do not\\ntest GPT-4 on the latter benchmark for the reasons previously mentioned, as well as the fact that the bench-\\nmark is visual in nature and thus more appropriate for the multimodal version of GPT-4 described in [Ope23].\\nTo overcome the limitations described above, we propose here a di\\x0berent approach to studying GPT-4\\nwhich is closer to traditional psychology rather than machine learning, leveraging human creativity and cu-\\nriosity. We aim to generate novel and di\\x0ecult tasks and questions that convincingly demonstrate that GPT-4\\ngoes far beyond memorization, and that it has a deep and \\rexible understanding of concepts, skills, and\\ndomains (a somewhat similar approach was also proposed in [CWF+22]). We also aim to probe GPT-4\\'s\\nresponses and behaviors, to verify its consistency, coherence, and correctness, and to uncover its limitations\\nand biases. We acknowledge that this approach is somewhat subjective and informal, and that it may not\\nsatisfy the rigorous standards of scienti\\x0cc evaluation. However, we believe that it is a useful and necessary\\n\\x0crst step to appreciate the remarkable capabilities and challenges of GPT-4, and that such a \\x0crst step opens\\nup new opportunities for developing more formal and comprehensive methods for testing and analyzing AI\\nsystems with more general intelligence.\\nTo illustrate our approach to assessing GPT-4\\'s intelligence, let us consider the \\x0crst two example inter-\\nactions with GPT-4 that we have in Figure 1.1. The \\x0crst example is asking GPT-4 to write a proof of the\\nin\\x0cnitude of primes in the form of a poem. This is a challenging task that requires combining elementary\\nmathematical reasoning, poetic expression, and natural language generation. The second example is asking\\nGPT-4 to draw a unicorn in TiKZ. This is another challenging task that requires combining visual imagina-\\ntion and coding skills. In both cases, GPT-4 produces impressive outputs that are far superior to those of\\nChatGPT, a previous state-of-the-art LLM, and at least comparable (if not superior) to what a human would\\ndo.\\n7', metadata={'source': 'data/SparkOfGPT-4.pdf', 'page': 6})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use llm to answers the query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.document_loaders import CSVLoader\n",
    "# from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(engine='gpt-4', temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qdocs} Question: What are the areas \\\n",
    "of improvement for GPT-4 over GPT-3.5?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "GPT-4 demonstrates significant improvements over GPT-3.5 in various aspects, including:\n",
       "\n",
       "1. Generality: GPT-4 shows a deeper and more flexible understanding of concepts, skills, and domains, enabling it to perform tasks that go beyond the typical scope of narrow AI systems.\n",
       "\n",
       "2. Problem-solving: GPT-4 exhibits better problem-solving abilities, including mathematical reasoning, coding, and planning.\n",
       "\n",
       "3. Human understanding: GPT-4 has a better grasp of human behavior, emotions, and communication, making it more effective in understanding and interacting with humans.\n",
       "\n",
       "4. Common sense: GPT-4 demonstrates improved common sense reasoning, which has been a challenge for previous language models.\n",
       "\n",
       "5. Truthfulness: GPT-4 generates more accurate and truthful responses compared to GPT-3.5, making it more reliable for various tasks.\n",
       "\n",
       "6. Performance on benchmarks: GPT-4 outperforms GPT-3.5 on several benchmarks, including those related to medicine, law, and software engineering.\n",
       "\n",
       "However, it is essential to note that GPT-4 still has limitations, such as the lack of an \"inner dialogue\" or \"scratchpad\" for multi-step computations and the inability to store intermediate results. Additionally, the autoregressive architecture of GPT-4 may limit its planning capabilities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
