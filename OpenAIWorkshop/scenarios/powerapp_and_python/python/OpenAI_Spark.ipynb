{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4c8776-6853-4257-bef8-72778724ad57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Azure OpenAI for Big Data\n",
    "\n",
    "The Azure OpenAI service can be used to solve a large number of natural language tasks through prompting the completion API. To make it easier to scale your prompting workflows from a few examples to large datasets of examples we have integrated the Azure OpenAI service with the distributed machine learning library [SynapseML](https://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/). This integration makes it easy to use the [Apache Spark](https://spark.apache.org/) distributed computing framework to process millions of prompts with the OpenAI service. This tutorial shows how to apply large language models at a distributed scale using Azure Open AI and Azure Synapse Analytics. \n",
    "\n",
    "## Step 1: Prerequisites\n",
    "\n",
    "The key prerequisites for this quickstart include a working Azure OpenAI resource, and an Apache Spark cluster with SynapseML installed. We suggest creating a Synapse workspace, but an Azure Databricks, HDInsight, or Spark on Kubernetes, or even a python environment with the `pyspark` package will work. \n",
    "\n",
    "1. An Azure OpenAI resource â€“ request access [here](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOFA5Qk1UWDRBMjg0WFhPMkIzTzhKQ1dWNyQlQCN0PWcu) before [creating a resource](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource)\n",
    "1. [Create a Synapse workspace](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-create-workspace) or Use Databricks\n",
    "1. [Create a serverless Apache Spark pool](https://docs.microsoft.com/en-us/azure/synapse-analytics/ get-started-analyze-spark#create-a-serverless-apache-spark-pool) or Use Databricks\n",
    "\n",
    "\n",
    "## Step 2: Import this guide as a notebook\n",
    "\n",
    "The next step is to add this code into your Spark cluster. You can either create a notebook in your Spark platform and copy the code into this notebook to run the demo. Or download the notebook and import it into Synapse Analytics\n",
    "\n",
    "1. Install SynapseML on your cluster. Please see the installation instructions for Synapse at the bottom of [the SynapseML website](https://microsoft.github.io/SynapseML/). Note that this requires pasting an additional cell at the top of the notebook you just imported\n",
    "3.\tConnect your notebook to a cluster and follow along, editing and runing the cells below.\n",
    "\n",
    "## Step 3: Fill in your service information\n",
    "\n",
    "Next, please edit the cell in the notebook to point to your service. In particular set the `service_name`, `deployment_name`, `location`, and `key` variables to match those for your OpenAI service. In case you are using AAD, you don't need to set the key but you need to install ```pip install azure-identity``` to run the authentication flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0db8af-7fe2-40bc-9df4-cc7f274d53f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from synapse.ml.core.platform import running_on_synapse, find_secret\n",
    "\n",
    "# Bootstrap Spark Session\n",
    "\n",
    "\n",
    "# Fill in the following lines with your service information\n",
    "service_name = \"\"\n",
    "deployment_name = \"gpt-35-turbo\"\n",
    "deployment_name_embeddings = \"text-embedding-ada-002\"\n",
    "deployment_name_embeddings_query = \"text-similarity-ada-001\"\n",
    "\n",
    "key = \"\"  # please replace this with your key as a string. \n",
    "\n",
    "assert key is not None and service_name is not None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb48578c-e4b3-49fc-9ee2-2f8ae8808c19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 4: Create the OpenAICompletion Apache Spark Client\n",
    "\n",
    "To apply the OpenAI Completion service to your dataframe you just created, create an OpenAICompletion object which serves as a distributed client. Parameters of the service can be set either with a single value, or by a column of the dataframe with the appropriate setters on the `OpenAICompletion` object. Here we are setting `maxTokens` to 2000. A token is around 4 characters, and this limit applies to the sum of the prompt and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dca7a9d-6092-48af-8653-10c141fd440d",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from synapse.ml.cognitive import OpenAICompletion\n",
    "\n",
    "completion = (\n",
    "    OpenAICompletion(stop= \"<|im_end|>\")\n",
    "    .setSubscriptionKey(key)\n",
    "    .setDeploymentName(deployment_name)\n",
    "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
    "    .setMaxTokens(2000)\n",
    "    .setPromptCol(\"prompt\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setOutputCol(\"completions\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For AAD authentication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Use DeviceCodeCredential to login interactively using user token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you use AAD as authenitcation method, make sure you are assigned Cognitive Services User role\n",
    "from azure.identity import DeviceCodeCredential\n",
    "from synapse.ml.cognitive import OpenAICompletion\n",
    "\n",
    "interactive_credential = DeviceCodeCredential() # add tenant_id=\"YOUR_TENANT_ID\" if you have more than 1 tenants\n",
    "token = interactive_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "completion = (\n",
    "    OpenAICompletion(stop= \"<|im_end|>\", AADToken=token.token)\n",
    "    .setDeploymentName(deployment_name)\n",
    "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
    "    .setMaxTokens(2000)\n",
    "    .setPromptCol(\"prompt\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setOutputCol(\"completions\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Use DefaultAzureCredential to acquire token using AAD Service Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_TENANT_ID\"] = TENANT_ID\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = OPENAI_APP_ID\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = OPENAI_APP_SECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "import openai\n",
    "\n",
    "# Request credential\n",
    "default_credential = DefaultAzureCredential()\n",
    "token = default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "completion = (\n",
    "    OpenAICompletion(stop= \"<|im_end|>\", AADToken=token.token)\n",
    "    .setDeploymentName(deployment_name)\n",
    "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
    "    .setMaxTokens(2000)\n",
    "    .setPromptCol(\"prompt\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setOutputCol(\"completions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76f069b7-14e8-44ea-97f0-1c49cf02eeed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 5: Create a dataset for summary\n",
    "\n",
    "Next, create a dataframe consisting of a series of rows, with one prompt per row. \n",
    "\n",
    "You can also load data directly from ADLS or other databases. For more information on loading and preparing Spark dataframes, see the [Apache Spark data loading guide](https://spark.apache.org/docs/latest/sql-data-sources.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9316daa-84cf-41a2-968f-8999cf06d190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, concat\n",
    "\n",
    "import pandas as pd\n",
    "request = \"Generate a customer support transcript between A HR/Payroll customer representative and an employee of a company. Just generate the transcript without adding any comment as the output is used by another system as is\"\n",
    "pandas_df = pd.DataFrame({\"prompt\":[f\"<|im_start|>system\\nAssistant is a large language model trained by OpenAI.\\n<|im_end|>\\n<|im_start|>user\\n{request}<|im_end|>\\n<|im_start|>assistant\\n\"]*6})\n",
    "data_df = spark.createDataFrame(pandas_df)\n",
    "completed_df = completion.transform(data_df).cache()\n",
    "\n",
    "out =completed_df.select(\n",
    "    col(\"completions.choices.text\").getItem(0).alias(\"text\"),\n",
    ")\n",
    "out.write.mode(\"overwrite\").saveAsTable(\"adp_customer_trans\")\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b2a454-65ab-45ec-9946-d92539899781",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 6: Transform the dataframe with the OpenAICompletion Client\n",
    "\n",
    "Now that you have the dataframe and the completion client, you can transform your input dataset and add a column called `completions` with all of the information the service adds. We will select out just the text for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8cf6a0-3b10-4f55-8906-7f84a0096f89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "request = \"summarize the following text\"\n",
    "sum_df = spark.table(\"adp_customer_trans\")\n",
    "sum_df = sum_df.withColumn('prompt',concat(lit(f\"<|im_start|>system\\nAssistant is a large language model trained by OpenAI.\\n<|im_end|>\\n<|im_start|>user\\n{request}:\\n\"), col('text'), lit(\"<|im_end|>\\n<|im_start|>assistant\\n\")))\n",
    "summary_result =completion.transform(sum_df.select(col(\"prompt\"))).cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed7d7ca-fd16-4caf-baeb-7fd971818cc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(summary_result.select(\n",
    "    col(\"completions.choices.text\").getItem(0).alias(\"summary\"),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9111da73-81f2-48e5-9a0c-eb65e1567f91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Additional Usage Examples\n",
    "\n",
    "### Improve throughput with request batching \n",
    "\n",
    "The example above makes several requests to the service, one for each prompt. To complete multiple prompts in a single request, use batch mode. First, in the OpenAICompletion object, instead of setting the Prompt column to \"Prompt\", specify \"batchPrompt\" for the BatchPrompt column.\n",
    "To do so, create a dataframe with a list of prompts per row.\n",
    "\n",
    "**Note** that as of this writing there is currently a limit of 20 prompts in a single request, as well as a hard limit of 2048 \"tokens\", or approximately 1500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0153ad4-769b-427d-ac7d-d7af2f407beb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chat_script = \"\"\"\n",
    "Customer: Hi, I'm having some issues with my payroll. Can you help me? \n",
    "Customer Service Representative: Yes, of course. Can you tell me more about the problem you are having? \n",
    "Customer: Well, my paycheck seems to be incorrect. I'm not sure what's going on. \n",
    "Customer Service Representative: I'm sorry to hear that. Let's take a look. Can you tell me your company name and your employee ID number? \n",
    "Customer: Sure. My company is XYZ Corporation, and my employee ID is 12345. \n",
    "Customer Service Representative: Thank you. Let me pull up your information. Okay, I see that you were paid last Friday. Is that the paycheck you are referring to? \n",
    "Customer: Yes, that's the one. \n",
    "Customer Service Representative: Okay, let me take a closer look. I see that your gross pay is correct, but there seem to be some deductions that are higher than usual. Can you tell me if anything has changed recently, like your tax withholding or benefits enrollment? \n",
    "Customer: No, I don't think so. I haven't made any changes to my benefits, and I don't remember changing my tax withholding either. \n",
    "Customer Service Representative: Okay, let me check your account settings. It looks like your tax withholding was changed to a higher rate last month. Did you make that change yourself? \n",
    "Customer: No, I didn't. I'm not sure how that could have happened. \n",
    "Customer Service Representative: It's possible that it was an error or a miscommunication with your HR department. I can help you correct it now. Can you confirm your current tax filing status and the number of allowances you want to claim? \n",
    "Customer: Yes, I'm single and I want to claim one allowance. \n",
    "Customer Service Representative: Okay, let me make that change for you. It should reflect on your next paycheck. Is there anything else I can help you with? \n",
    "Customer: Actually, while we're on the topic of taxes, I have a question about my W-2 form. \n",
    "Customer Service Representative: Sure, what's your question? \n",
    "Customer: I received my W-2 form in the mail, but I noticed that my social security number is incorrect. What should I do? \n",
    "Customer Service Representative: Oh no, I'm sorry to hear that. We will need to correct that as soon as possible. Can you confirm your correct social security number? \n",
    "Customer: Yes, it's 123-45-6789. \n",
    "Customer Service Representative: Thank you. I will update your records and issue a corrected W-2 form to you. Is there anything else you need assistance with today? \n",
    "Customer: Actually, yes. I was supposed to receive a bonus in my last paycheck, but I didn't see it on there. Can you help me figure out what happened? \n",
    "Customer Service Representative: Yes, let me check your payroll history. Okay, it looks like the bonus was not included in your last paycheck. I apologize for the error. I can process the bonus payment for you now and it should be reflected on your next paycheck. \n",
    "Customer: Thank you, I appreciate that. I also have a question about my 401(k) plan. How do I enroll in the plan and make changes to my contributions? \n",
    "Customer Service Representative: Great question. To enroll in the plan, you will need to speak with your HR department and fill out the necessary forms. Once you are enrolled, you can make changes to your contributions by logging in to your ADP account online or through our mobile app. Would you like me to assist you with that? \n",
    "Customer: Yes, please. \n",
    "(Customer Service Representative walks the customer through the process of logging in and making changes to their 401(k) contributions) \n",
    "Customer Service Representative: Okay, your changes have been saved. Is there anything else I can help you with today? \n",
    "Customer: Yes, actually. I have a question about my time off accrual. How do I check how much time I have left? \n",
    "Customer Service Representative: That's a great question. You can check your time off accrual and balance by logging in to your ADP account and viewing your pay statements. Your time off balance should be listed there. Do you need help logging in? \n",
    "Customer: No, I think I can handle that. Thank you. \n",
    "Customer Service Representative: You're welcome. Is there anything else I can help you with today? \n",
    "Customer: Yes, I have a question about my direct deposit. Can I split my paycheck between two bank accounts? \n",
    "Customer Service Representative: Yes, you can split your paycheck between two or more bank accounts. You will need to speak with your HR department to set up the split deposit and provide them with the necessary information for each account. Once the split deposit is set up, it should be reflected on your next paycheck. Is there anything else I can help you with today? \n",
    "Customer: No, that's all for now. Thank you for your help. \n",
    "Customer Service Representative: You're welcome. Don't hesitate to call us back if you have any more questions or concerns. Have a great day!\n",
    "\"\"\"\n",
    "pandas_df = pd.DataFrame({\"batchPrompt\":[[f\"<|im_start|>system\\nAssistant is a large language model trained by OpenAI.\\n<|im_end|>\\n<|im_start|>user\\nSummarize the following conversation\\n{item}<|im_end|>\\n<|im_start|>assistant\\n\" for item in [chat_script]*5]]})\n",
    "batch_df = spark.createDataFrame(pandas_df)\n",
    "display(batch_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb5daf9-8155-460c-b2dd-e1ca302a3776",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we create the OpenAICompletion object. Rather than setting the prompt column, set the batchPrompt column if your column is of type `Array[String]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8411e5ba-7f22-4ac9-a78e-1746a7ccc8bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_completion = (\n",
    "    OpenAICompletion()\n",
    "    .setSubscriptionKey(key)\n",
    "    .setDeploymentName(deployment_name)\n",
    "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
    "    .setMaxTokens(2000)\n",
    "    .setBatchPromptCol(\"batchPrompt\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setOutputCol(\"completions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you use AAD as authenitcation method, make sure you are assigned Cognitive Services User role\n",
    "from synapse.ml.cognitive import OpenAICompletion\n",
    "\n",
    "batch_completion = (\n",
    "    OpenAICompletion(stop= \"<|im_end|>\", AADToken=token.token)\n",
    "    .setDeploymentName(deployment_name)\n",
    "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
    "    .setMaxTokens(2000)\n",
    "    .setBatchPromptCol(\"batchPrompt\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setOutputCol(\"completions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be2a0d15-40e1-4a3d-a879-d7d0e0129b35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the call to transform a request will then be made per row. Since there are multiple prompts in a single row, each request will be sent with all prompts in that row. The results will contain a row for each row in the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6fb7509-f582-47bd-8b57-f59d51c03eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completed_batch_df = batch_completion.transform(batch_df)\n",
    "display(completed_batch_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dd7259b-173a-41ef-b98e-7b1dc0df875f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Using an automatic minibatcher\n",
    "\n",
    "If your data is in column format, you can transpose it to row format using SynapseML's `FixedMiniBatcherTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04212778-8002-4e30-bf31-b7511c5776fd",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from synapse.ml.stages import FixedMiniBatchTransformer\n",
    "from synapse.ml.core.spark import FluentAPI\n",
    "sum_df = spark.table(\"adp_customer_trans\")\n",
    "sum_df = sum_df.withColumn('prompt',concat(lit(f\"<|im_start|>system\\nAssistant is a large language model trained by OpenAI.\\n<|im_end|>\\n<|im_start|>user\\n{request}:\\n\"), col('text'), lit(\"<|im_end|>\\n<|im_start|>assistant\\n\")))\n",
    "\n",
    "completed_autobatch_df = (\n",
    "    sum_df.coalesce(\n",
    "        2\n",
    "    )  # Force a single partition so that our little 6-row dataframe makes a batch of size 3, you can remove this step for large datasets\n",
    "    .mlTransform(FixedMiniBatchTransformer(batchSize=3))\n",
    "    .withColumnRenamed(\"prompt\", \"batchPrompt\")\n",
    "    .mlTransform(batch_completion)\n",
    ")\n",
    "\n",
    "display(completed_autobatch_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d90ac8e-dbd4-4b02-8af4-6f949b84037d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# OpenAI Embeddings\n",
    "\n",
    "We will use t-SNE to reduce the dimensionality of the embeddings from 1536 to 2. Once the embeddings are reduced to two dimensions, we can plot them in a 2D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32853189-10c4-41ba-9e80-3c7c003c4ac2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from synapse.ml.cognitive import OpenAIEmbedding\n",
    "\n",
    "embedding = (\n",
    "    OpenAIEmbedding()\n",
    "    .setSubscriptionKey(key)\n",
    "    .setDeploymentName(deployment_name_embeddings)\n",
    "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
    "    .setTextCol(\"combined\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setOutputCol(\"embeddings\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85824f5c-af6c-4ee1-8e87-f0f007f8a93a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.options(inferSchema=\"True\", delimiter=\",\", header=True).csv(\n",
    "    \"wasbs://publicwasb@mmlspark.blob.core.windows.net/fine_food_reviews_1k.csv\"\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"combined\",\n",
    "    F.format_string(\"Title: %s; Content: %s\", F.trim(df.Summary), F.trim(df.Text)),\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa5c4eb-e396-4550-bc57-4ab5259ea512",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "completed_df = embedding.transform(df).cache()\n",
    "display(completed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f504d3be-a454-4173-951f-c648a3c489a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Retrieve embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01b29ee-6939-4ef1-bf6e-397d736670bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.array(completed_df.select(\"embeddings\").collect())[:, 0, :]\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6a6a4df-c02a-44ba-8f46-27460ba0a9ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reduce dimensionality\n",
    "We reduce the dimensionality to 2 dimensions using t-SNE decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a4aac8-5fcc-427c-adbf-85a68175771e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Create a t-SNE model and transform the data\n",
    "tsne = TSNE(\n",
    "    n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200\n",
    ")\n",
    "vis_dims = tsne.fit_transform(matrix)\n",
    "vis_dims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73f38000-2d06-479c-aa32-326b4c96ee7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Plot the embeddings\n",
    "We colour each review by its star rating, ranging from red for negative reviews, to green for positive reviews..\n",
    "\n",
    "We can observe a decent data separation even in the reduced 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "122ee39c-743e-4d2c-8ba1-7d5f8beb94e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "scores = np.array(completed_df.select(\"Score\").collect()).reshape(-1)\n",
    "\n",
    "colors = [\"red\", \"darkorange\", \"gold\", \"turquoise\", \"darkgreen\"]\n",
    "x = [x for x, y in vis_dims]\n",
    "y = [y for x, y in vis_dims]\n",
    "color_indices = scores - 1\n",
    "\n",
    "colormap = matplotlib.colors.ListedColormap(colors)\n",
    "plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\n",
    "for score in [0, 1, 2, 3, 4]:\n",
    "    avg_x = np.array(x)[scores - 1 == score].mean()\n",
    "    avg_y = np.array(y)[scores - 1 == score].mean()\n",
    "    color = colors[score]\n",
    "    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "\n",
    "plt.title(\"Amazon ratings visualized in language using t-SNE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fc07cbe-ae93-424f-9931-1da542123d4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Use embeddings to build a semantic search Index\n",
    "\n",
    "Note that for some OpenAI models, users should use separate models for embedding documents and queries. These models are denoted by the \"-doc\" and \"-query\" suffixes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1826bb-b738-4202-88ae-6d3f60199345",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_query = (\n",
    "    OpenAIEmbedding()\n",
    "    .setSubscriptionKey(key)\n",
    "    .setDeploymentName(deployment_name_embeddings_query)\n",
    "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
    "    .setTextCol(\"query\")\n",
    "    .setErrorCol(\"error\")\n",
    "    .setOutputCol(\"embeddings\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b791b138-c1ee-4507-9366-4c40aadf57cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create a dataframe of search queries\n",
    "\n",
    "Note: The data types of the ID columns in the document and query dataframes should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068240d3-bf59-44ca-86d2-39a5cea7f1b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_df = (\n",
    "    spark.createDataFrame(\n",
    "        [\n",
    "            (\n",
    "                0,\n",
    "                \"desserts\",\n",
    "            ),\n",
    "            (\n",
    "                1,\n",
    "                \"disgusting\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .toDF(\"id\", \"query\")\n",
    "    .withColumn(\"id\", F.col(\"id\").cast(\"int\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ace16c1-2ba4-47ac-b9f8-b5f98881ff64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate embeddings for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62ff3fd2-2a1e-4d24-8fb1-66e4e3141ff2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completed_query_df = embedding_query.transform(query_df).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6185aec-f0eb-4c4a-9811-d7acfb11d342",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Build index for fast retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8884fa79-326c-4ab9-9171-ff7a60b84b87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from synapse.ml.nn import *\n",
    "\n",
    "knn = (\n",
    "    KNN()\n",
    "    .setFeaturesCol(\"embeddings\")\n",
    "    .setValuesCol(\"id\")\n",
    "    .setOutputCol(\"output\")\n",
    "    .setK(10)\n",
    ")  # top-k for retrieval\n",
    "\n",
    "knn_index = knn.fit(completed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0efefb81-d63d-438f-9278-6763d8a4bb79",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Retrieve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc724d6-c240-4936-b463-88f5424b2bb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_matches = knn_index.transform(completed_query_df).cache()\n",
    "\n",
    "df_result = (\n",
    "    df_matches.withColumn(\"match\", F.explode(\"output\"))\n",
    "    .join(df, df[\"id\"] == F.col(\"match.value\"))\n",
    "    .select(\"query\", F.col(\"combined\"), \"match.distance\")\n",
    ")\n",
    "\n",
    "display(df_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 449370649715216,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "CognitiveServices - OpenAI",
   "notebookOrigID": 449370649715169,
   "widgets": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {
    "4bd0e60b-98ae-4bfe-98ee-6f0399ceb456": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "count",
        "categoryFieldKeys": [
         "0"
        ],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [
         "0"
        ]
       },
       "tableOptions": {},
       "type": "details"
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "Once upon a time",
         "1": [
          " there was a girl who had a dream of becoming a writer.\n\nShe started writing short stories"
         ]
        },
        {
         "0": "Hello my name is",
         "1": [
          "***** and I have a question about my cat\n\nHello, thank you for bringing your question to"
         ]
        },
        {
         "0": "The best code is code thats",
         "1": [
          " not there\n\nCommenting your code is important. Not only does it help you remember what you"
         ]
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "prompt",
         "type": "string"
        },
        {
         "key": "1",
         "name": "text",
         "type": "ArrayType(StringType,true)"
        }
       ],
       "truncated": false
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
