{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "API_KEY = \"\"\n",
    "RESOURCE_ENDPOINT = \"\" \n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation (Skip this step if data is already generated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\n",
    "def generate_titles():\n",
    "    user_message =f\"\"\" \n",
    "    generate 100 titles of customer support articles in your industry, focusing on the areas of payroll and HR that support agent can look up to answer questions from customers who are employees with payroll managed by ADP.\n",
    "    Output data into a list \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a customer support agent for ADP company\"},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "            \n",
    "\n",
    "titles = generate_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = titles.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_article(title):\n",
    "    user_message =f\"\"\" \n",
    "    given this title of an article \"{title}\" that support agents can look up to answer questions from customers who are employees with payroll managed by ADP.\n",
    "    Generate detail content of the article \n",
    " \n",
    "\"\"\"\n",
    "    i=0\n",
    "    while i<5:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a customer support agent for ADP company\"},\n",
    "                    {\"role\": \"user\", \"content\":user_message },\n",
    "                ]\n",
    "            )\n",
    "            return response['choices'][0]['message']['content']\n",
    "        except:\n",
    "            i+=1\n",
    "            time.sleep(5)\n",
    "            \n",
    "\n",
    "contents =[]\n",
    "for title in titles:\n",
    "    contents.append(generate_article(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path =\"../../../data/agent_assistant\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "i=0\n",
    "articles ={}\n",
    "for title, content in zip(titles, contents):\n",
    "    file_name = f\"article_{i}\"\n",
    "    file_content = title+\"\\n\\n\"+content\n",
    "\n",
    "    with open(os.path.join(folder_path,file_name), 'w') as file:\n",
    "        file.write(file_content)\n",
    "\n",
    "    articles[file_name]= file_content\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Enrichment with GPT\n",
    "To help semantic search better, we can run GPT models (ChatGPT, GPT-4, GPT-4-32k) through knowledge articles to create topics out of each document together with original content. Basically, the idea is to break the raw article content into topics with clear description so that search can better map question/query with right document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "import json\n",
    "import time\n",
    "gpt_turbo_model = \"gpt-35-turbo\"\n",
    "gpt_4_32k_model = \"\" #find name of gpt_4_32k model\n",
    "\n",
    "def enrich(article_content):\n",
    "    user_message=f\"\"\" \n",
    "    Given the document below, extract key topics, each with a short description and corresponding extracted original content from the document. \n",
    "    Output data in a list with this format:\n",
    "    [{{\"topic\": \"Topic Name\", \"description\": \"description of topic\", \"extracted_content\": \"original content extracted from the document\"}}]\n",
    "    Just output data, do not add any comment.\n",
    "    <<document>>\n",
    "    {article_content}\n",
    "    <<document>>\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    system_message = \"\"\"\n",
    "    You are an AI assistant that helps people organize data. \n",
    "    \"\"\"\n",
    "    gpt_model= gpt_turbo_model #default choice, it can be better to go with GPT-4\n",
    "    if len(article_content)>14500: #if the count of document character > 2x the limit of GPT_turbo, go with GPT-4-32K-model\n",
    "        gpt_model= gpt_4_32k_model\n",
    "        print(f\"using gpt-4-32k model named {gpt_model} for this enrichment\")\n",
    "\n",
    "    i=0\n",
    "    output=\"\"\n",
    "    while i<10: #if the output format is not as expected or if there's a throttling error, retry up to 10 times.\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=gpt_model, \n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\":user_message },\n",
    "                ]\n",
    "            )\n",
    "            response=response['choices'][0]['message']['content']\n",
    "            output = json.loads(response)\n",
    "            for content in output:\n",
    "                content[\"topic\"],content[\"description\"],content[\"extracted_content\"] #this is just to validate the format\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"temporary exception occured, will retry after 3s\", str(e))\n",
    "            i+=1\n",
    "            time.sleep(3)\n",
    "    if len(\"output\")==0:\n",
    "        raise Exception(\"Cannot extract the content after retrying 10 times\")\n",
    "    return output\n",
    "enriched_articles =[]\n",
    "for article_file, article_content in articles.items():\n",
    "    enriched_article ={}\n",
    "    try:\n",
    "        enriched_article[\"enriched_content\"] = enrich(article_content)\n",
    "    except Exception:\n",
    "        print(f\"Warning, document {article_file} cannot be enriched and is not included in the result, please check\")\n",
    "    enriched_article[\"article_file\"] = article_file\n",
    "    enriched_article[\"article_content\"] = article_content\n",
    "\n",
    "    enriched_articles.append(enriched_article)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building content map for topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2022-12-01\"\n",
    "topic_content={}\n",
    "for enriched_article in enriched_articles:\n",
    "    topics = enriched_article['enriched_content']\n",
    "    for topic in topics:\n",
    "        article_topic_id= enriched_article['article_file']+\"###\"+ topic['topic']\n",
    "        topic_content[article_topic_id]=topic['extracted_content']\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2022-12-01\"\n",
    "enriched_emb={}\n",
    "for enriched_article in enriched_articles:\n",
    "    topics = enriched_article['enriched_content']\n",
    "    for topic in topics:\n",
    "        article_topic_id= enriched_article['article_file']+\"###\"+ topic['topic']\n",
    "        topic_emb =get_embedding(topic['topic']+\"\\n\"+topic['description'], engine = 'text-embedding-ada-002')\n",
    "        content_emb = get_embedding(topic['topic']+\"\\n\"+topic['description']+\"\\n\"+topic['extracted_content'], engine = 'text-embedding-ada-002')\n",
    "        enriched_emb[article_topic_id]=(topic_emb,content_emb)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "folder_path =\"../../data/agent_assistant\"\n",
    "\n",
    "with open(os.path.join(folder_path,\"enriched_emb.json\"), \"w\") as fp:\n",
    "    json.dump(enriched_emb,fp) \n",
    "\n",
    "with open(os.path.join(folder_path,\"enriched_articles.json\"), \"w\") as fp:\n",
    "    json.dump(enriched_articles,fp) \n",
    "with open(os.path.join(folder_path,\"topic_content.json\"), \"w\") as fp:\n",
    "    json.dump(topic_content,fp) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistant Design"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "folder_path =\"../../data/agent_assistant\"\n",
    "with open(os.path.join(folder_path, \"enriched_emb.json\"), \"r\") as file:\n",
    "    enriched_emb = json.load(file)\n",
    "with open(os.path.join(folder_path, \"enriched_articles.json\"), \"r\") as file:\n",
    "    enriched_articles = json.load(file)\n",
    "with open(os.path.join(folder_path, \"topic_content.json\"), \"r\") as file:\n",
    "    topic_content = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tool to generate conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "def generate_conversation(streaming =False):\n",
    "    user_message =f\"\"\" \n",
    "    Generate a phone conversation between an customer's employee and an ADP support agent in the area of HR and Payroll. The employee has question and the support agent tried to identity the problems and \n",
    "    and take time to use different search tool to come up with answer to the employee.\n",
    "    Your response:\n",
    " \n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a customer support agent for ADP company. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ],\n",
    "        stream=streaming\n",
    "    )\n",
    "    return response\n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tool to extract problems from conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "user_message = \"\"\n",
    "def extract_problems(conversation):\n",
    "    user_message =f\"\"\" \n",
    "    Follow this on going conversation below and extract problems that each party may need help with and formulate the search query to the knowledge base search tool.\n",
    "    <<conversattion>>\n",
    "    {conversation}\n",
    "    <<conversattion>>\n",
    "    Output your response in JSON format {{\"problem\":\"summary of problem\", \"search_query\":\"content of search query\"}}\n",
    "    There can be more than 1 problem(s)\n",
    "    Output just JSON, nothing else.\n",
    "    Your response:\n",
    " \n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a senior customer support agent for ADP company. You listen to the conversation between an agent and a customer and assist the agent to resolve the problem.\n",
    "    You are given access to knowledge base search tool to find knowledge needed to find answer to questions. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        timeout=100,\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "            \n",
    "conversation=generate_conversation()['choices'][0]['message']['content']\n",
    "problems=extract_problems(conversation)\n",
    "problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tool to find article given a problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brute Force method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "def find_article(question, topk=5):  \n",
    "    \"\"\"  \n",
    "    Given an input vector and a dictionary of label vectors,  \n",
    "    returns the label with the highest cosine similarity to the input vector.  \n",
    "    \"\"\"  \n",
    "    input_vector = get_embedding(question, engine = 'text-embedding-ada-002')\n",
    "    best_file_name = None  \n",
    "      \n",
    "    # Compute cosine similarity between input vector and each label vector\n",
    "    cosine_list=[]  \n",
    "    for topic_id, vector in enriched_emb.items():  \n",
    "        #by default, we use embedding for the entire content of the topic (plus topic descrition).\n",
    "        # If you you want to use embedding on just topic name and description use this code cosine_sim = cosine_similarity(input_vector, vector[0])\n",
    "        cosine_sim = cosine_similarity(input_vector, vector[1]) \n",
    "        cosine_list.append((topic_id,cosine_sim ))\n",
    "    cosine_list.sort(key=lambda x:x[1],reverse=True)\n",
    "    cosine_list= cosine_list[:topk]\n",
    "    print(cosine_list)\n",
    "    best_topics = [topic[0] for topic in cosine_list]\n",
    "    article_files =[best_topic.split(\"###\")[0] for best_topic in best_topics]\n",
    "    contents = [topic_content[best_topic] for best_topic in best_topics]\n",
    "    return article_files, contents\n",
    "question = \"Why my paycheck is smaller than usual\"\n",
    "find_article(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Searh Lib (Faiss) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import faiss\n",
    "\n",
    "openai.api_version = \"2022-12-01\"\n",
    "#Get the array of embeddings for all articles\n",
    "topic_ids =[]\n",
    "emb_vectors = []\n",
    "for topic_id, vector in enriched_emb.items():  \n",
    "    #by default, we use embedding for the entire content of the topic (plus topic descrition).\n",
    "    # If you you want to use embedding on just topic name and description use this code cosine_sim = cosine_similarity(input_vector, vector[0])\n",
    "    topic_ids.append(topic_id)\n",
    "    emb_vectors.append(vector[1])\n",
    "emb_vectors = np.float32(emb_vectors)\n",
    "faiss.normalize_L2(emb_vectors)\n",
    "\n",
    "index = faiss.IndexFlatIP(len(vector[1])) \n",
    "index.add(emb_vectors)\n",
    "\n",
    "\n",
    "def find_article_emb_vec(question, topk=3):  \n",
    "    \"\"\"  \n",
    "    Given an input vector and a dictionary of label vectors,  \n",
    "    returns the label with the highest cosine similarity to the input vector.  \n",
    "    \"\"\"  \n",
    "    input_vector = get_embedding(question, engine = 'text-embedding-ada-002')\n",
    "    input_vector = np.float32([input_vector])\n",
    "    faiss.normalize_L2(input_vector)\n",
    "    d,i = index.search(input_vector, k=topk)\n",
    "    best_topics = [topic_ids[idx] for idx in i[0]]\n",
    "    print(d)\n",
    "    output_contents=[topic_content[best_topic] for best_topic in best_topics]\n",
    "    article_files =[best_topic.split(\"###\")[0] for best_topic in best_topics]\n",
    "    return article_files, output_contents\n",
    "question = \"Why my paycheck is smaller than usual\"\n",
    "find_article_emb_vec(question,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tool to answer question from the given problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "def answer_assist(problem, search_query):\n",
    "\n",
    "    articles, contents = find_article(search_query,2)\n",
    "    articles_contents=\"\"\n",
    "    for article, content in zip(articles, contents):\n",
    "        articles_contents += f\"\"\" \n",
    "        article:{article}\n",
    "        content: {content}\n",
    "    \"\"\"\n",
    "    articles_contents = f\"\"\"\n",
    "    <<knowledge articles>>\n",
    "    {articles_contents}\n",
    "    <<knowledge articles>>\n",
    "    \"\"\"\n",
    "    user_message =f\"\"\" \n",
    "    problem:{problem}\n",
    "    {articles_contents}\n",
    "    Your response:\n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a senior customer support agent for ADP company. You listen to the conversation between an agent and a customer and assist the agent to resolve the problem.\n",
    "    Given the question or problem statement and the knowledge article you have, give the answer.\n",
    "    Rely solely to the guidance from the article.If the knowlege article is not relavant to the question, say you don't know. Do not make up your answer. \n",
    "    Cite the name of the knowledge article as source for your answer.\n",
    "    Format:\n",
    "    Answer: your answer to the question\n",
    "    Sources: [source1, source2]\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'], articles_contents\n",
    "\n",
    "question = \"When do I receive my W2 form?\"\n",
    "answer, articles_contents = answer_assist(question,question)\n",
    "print(answer)\n",
    "print(\"---------------content-----------------\")\n",
    "\n",
    "print(articles_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Put tools all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(conversation):\n",
    "    i=0\n",
    "    while (i<5): #handle wrong format output\n",
    "        problems=extract_problems(conversation)\n",
    "        try:\n",
    "            problems=json.loads(problems)\n",
    "            print(\"problems\", problems)\n",
    "            for problem in problems:\n",
    "                answer, articles_contents = answer_assist(problem['problem'],problem['search_query'])\n",
    "                print(answer)\n",
    "                print(\"---------------content-----------------\")\n",
    "\n",
    "                print(articles_contents)\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"problem parsing json, problems string is \", problems)\n",
    "            i+=1\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One time conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation =generate_conversation()['choices'][0]['message']['content']\n",
    "print(f\"Conversation {conversation}\")\n",
    "recommend(conversation)        \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a running conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "openai.api_version = \"2023-05-15\"\n",
    "import time\n",
    "conversation_pause_duration=2\n",
    "agent_assist_freq=5\n",
    "conversation_buffer =[]\n",
    "def stream_conversation(conversation_buffer, pause_duration=5):\n",
    "    responses = generate_conversation(True)\n",
    "    conversation_counter =0\n",
    "    old_conversation_counter =0\n",
    "    for response in responses:\n",
    "        content = response['choices'][0][\"delta\"].get(\"content\",\"\")\n",
    "        conversation_buffer.append(content) \n",
    "        if \"\\n\"  not in content:\n",
    "            print(content, end=\"\")\n",
    "        else:\n",
    "            conversation_counter+=1\n",
    "            if conversation_counter > old_conversation_counter+2:\n",
    "                conversation = \"\".join(conversation_buffer)\n",
    "                print(\"starting recommendation\")\n",
    "                print(\"conversation: \", conversation)\n",
    "                recommend(conversation)\n",
    "                print(\"ending recommendation\")\n",
    "                old_conversation_counter=conversation_counter\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "stream_conversation(conversation_buffer,conversation_pause_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
