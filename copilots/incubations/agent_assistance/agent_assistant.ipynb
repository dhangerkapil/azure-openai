{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "API_KEY = \"\"\n",
    "RESOURCE_ENDPOINT = \"\" \n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation (Skip this step if data is already generated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\n",
    "def generate_titles():\n",
    "    user_message =f\"\"\" \n",
    "    generate 100 titles of customer support articles in your industry, focusing on the areas of payroll and HR that support agent can look up to answer questions from customers who are employees with payroll managed by ADP.\n",
    "    Output data into a list \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a customer support agent for ADP company\"},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "            \n",
    "\n",
    "titles = generate_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = titles.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_article(title):\n",
    "    user_message =f\"\"\" \n",
    "    given this title of an article \"{title}\" that support agents can look up to answer questions from customers who are employees with payroll managed by ADP.\n",
    "    Generate detail content of the article \n",
    " \n",
    "\"\"\"\n",
    "    i=0\n",
    "    while i<5:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a customer support agent for ADP company\"},\n",
    "                    {\"role\": \"user\", \"content\":user_message },\n",
    "                ]\n",
    "            )\n",
    "            return response['choices'][0]['message']['content']\n",
    "        except:\n",
    "            i+=1\n",
    "            time.sleep(5)\n",
    "            \n",
    "\n",
    "contents =[]\n",
    "for title in titles:\n",
    "    contents.append(generate_article(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path =\"../../data/agent_assistant\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "i=0\n",
    "articles ={}\n",
    "for title, content in zip(titles, contents):\n",
    "    file_name = f\"article_{i}\"\n",
    "    file_content = title+\"\\n\\n\"+content\n",
    "\n",
    "    with open(os.path.join(folder_path,file_name), 'w') as file:\n",
    "        file.write(file_content)\n",
    "\n",
    "    articles[file_name]= file_content\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "articles_emb = {article[0]:get_embedding(article[1], engine = 'text-embedding-ada-002')for article in list(articles.items())}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(folder_path,\"articles_emb.json\"), \"w\") as fp:\n",
    "    json.dump(articles_emb,fp) \n",
    "\n",
    "with open(os.path.join(folder_path,\"articles.json\"), \"w\") as fp:\n",
    "    json.dump(articles,fp) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistant Design"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "folder_path =\"../../data/agent_assistant\"\n",
    "with open(os.path.join(folder_path, \"articles_emb.json\"), \"r\") as file:\n",
    "    articles_emb = json.load(file)\n",
    "with open(os.path.join(folder_path, \"articles.json\"), \"r\") as file:\n",
    "    articles = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tool to generate conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "def generate_conversation(streaming =False):\n",
    "    user_message =f\"\"\" \n",
    "    Generate a phone conversation between an customer's employee and an ADP support agent in the area of HR and Payroll. The employee has question and the support agent tried to identity the problems and \n",
    "    and take time to use different search tool to come up with answer to the employee.\n",
    "    Your response:\n",
    " \n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a customer support agent for ADP company. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ],\n",
    "        stream=streaming\n",
    "    )\n",
    "    return response\n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tool to extract problems from conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "user_message = \"\"\n",
    "def extract_problems(conversation):\n",
    "    user_message =f\"\"\" \n",
    "    Follow this on going conversation below and extract problems that each party may need help with and formulate the search query to the knowledge base search tool.\n",
    "    <<conversattion>>\n",
    "    {conversation}\n",
    "    <<conversattion>>\n",
    "    Output your response in JSON format {{\"problem\":\"summary of problem\", \"search_query\":\"content of search query\"}}\n",
    "    There can be more than 1 problem(s)\n",
    "    Output just JSON, nothing else.\n",
    "    Your response:\n",
    " \n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a senior customer support agent for ADP company. You listen to the conversation between an agent and a customer and assist the agent to resolve the problem.\n",
    "    You are given access to knowledge base search tool to find knowledge needed to find answer to questions. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        timeout=100,\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "            \n",
    "conversation=generate_conversation()['choices'][0]['message']['content']\n",
    "problems=extract_problems(conversation)\n",
    "problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tool to find article given a problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brute Force method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "def find_article(question, topk=5):  \n",
    "    \"\"\"  \n",
    "    Given an input vector and a dictionary of label vectors,  \n",
    "    returns the label with the highest cosine similarity to the input vector.  \n",
    "    \"\"\"  \n",
    "    input_vector = get_embedding(question, engine = 'text-embedding-ada-002')\n",
    "    max_similarity = float('-inf')  \n",
    "    best_file_name = None  \n",
    "      \n",
    "    # Compute cosine similarity between input vector and each label vector\n",
    "    cosine_list=[]  \n",
    "    for file_name, vector in articles_emb.items():  \n",
    "        cosine_sim = cosine_similarity(input_vector, vector) \n",
    "        cosine_list.append((file_name,cosine_sim ))\n",
    "    cosine_list.sort(key=lambda x:x[1],reverse=True)\n",
    "    cosine_list= cosine_list[:topk]\n",
    "    print(cosine_list)\n",
    "    best_file_names = [file_name[0] for file_name in cosine_list]\n",
    "    contents = [articles[best_file_name] for best_file_name in best_file_names]\n",
    "    return best_file_names, contents\n",
    "question = \"How do I update employee information\"\n",
    "find_article(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Searh Lib (Faiss) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import faiss\n",
    "\n",
    "openai.api_version = \"2022-12-01\"\n",
    "#Get the array of embeddings for all articles\n",
    "file_names =[]\n",
    "emb_vectors = []\n",
    "for file_name, vector in articles_emb.items():  \n",
    "    file_names.append(file_name)\n",
    "    emb_vectors.append(vector)\n",
    "emb_vectors = np.float32(emb_vectors)\n",
    "faiss.normalize_L2(emb_vectors)\n",
    "\n",
    "index = faiss.IndexFlatIP(len(vector)) \n",
    "index.add(emb_vectors)\n",
    "\n",
    "\n",
    "def find_article_emb_vec(question, topk=3):  \n",
    "    output_articles=[]\n",
    "    output_contents =[]\n",
    "    \"\"\"  \n",
    "    Given an input vector and a dictionary of label vectors,  \n",
    "    returns the label with the highest cosine similarity to the input vector.  \n",
    "    \"\"\"  \n",
    "    input_vector = get_embedding(question, engine = 'text-embedding-ada-002')\n",
    "    input_vector = np.float32([input_vector])\n",
    "    faiss.normalize_L2(input_vector)\n",
    "    d,i = index.search(input_vector, k=topk)\n",
    "    print(d)\n",
    "    for idx in i[0]:\n",
    "        output_articles.append(file_names[idx])\n",
    "        output_contents.append(articles[file_names[idx]])\n",
    "\n",
    "    return output_articles, output_contents\n",
    "question = \"How do I update employee information\"\n",
    "find_article_emb_vec(question,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tool to answer question from the given problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "def answer_assist(problem, search_query):\n",
    "\n",
    "    articles, contents = find_article_emb_vec(search_query,2)\n",
    "    articles_contents=\"\"\n",
    "    for article, content in zip(articles, contents):\n",
    "        articles_contents += f\"\"\" \n",
    "        article:{article}\n",
    "        content: {content}\n",
    "    \"\"\"\n",
    "    articles_contents = f\"\"\"\n",
    "    <<knowledge articles>>\n",
    "    {articles_contents}\n",
    "    <<knowledge articles>>\n",
    "    \"\"\"\n",
    "    user_message =f\"\"\" \n",
    "    problem:{problem}\n",
    "    {articles_contents}\n",
    "    Your response:\n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a senior customer support agent for ADP company. You listen to the conversation between an agent and a customer and assist the agent to resolve the problem.\n",
    "    Given the question or problem statement and the knowledge article you have, give the answer.\n",
    "    Rely solely to the guidance from the article.If the knowlege article is not relavant to the question, say you don't know. Do not make up your answer. \n",
    "    Cite the name of the knowledge article as source for your answer.\n",
    "    Format:\n",
    "    Answer: your answer to the question\n",
    "    Sources: [source1, source2]\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'], articles_contents\n",
    "\n",
    "question = \"When do I receive my W2 form?\"\n",
    "answer, articles_contents = answer_assist(question,question)\n",
    "print(answer)\n",
    "print(\"---------------content-----------------\")\n",
    "\n",
    "print(articles_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Put tools all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(conversation):\n",
    "    i=0\n",
    "    while (i<5): #handle wrong format output\n",
    "        problems=extract_problems(conversation)\n",
    "        try:\n",
    "            problems=json.loads(problems)\n",
    "            print(\"problems\", problems)\n",
    "            for problem in problems:\n",
    "                answer, articles_contents = answer_assist(problem['problem'],problem['search_query'])\n",
    "                print(answer)\n",
    "                print(\"---------------content-----------------\")\n",
    "\n",
    "                print(articles_contents)\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"problem parsing json, problems string is \", problems)\n",
    "            i+=1\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One time conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation =generate_conversation()['choices'][0]['message']['content']\n",
    "print(f\"Conversation {conversation}\")\n",
    "recommend(conversation)        \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a running conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "openai.api_version = \"2023-05-15\"\n",
    "import time\n",
    "conversation_pause_duration=2\n",
    "agent_assist_freq=5\n",
    "conversation_buffer =[]\n",
    "def stream_conversation(conversation_buffer, pause_duration=5):\n",
    "    responses = generate_conversation(True)\n",
    "    conversation_counter =0\n",
    "    old_conversation_counter =0\n",
    "    for response in responses:\n",
    "        content = response['choices'][0][\"delta\"].get(\"content\",\"\")\n",
    "        conversation_buffer.append(content) \n",
    "        if \"\\n\"  not in content:\n",
    "            print(content, end=\"\")\n",
    "        else:\n",
    "            conversation_counter+=1\n",
    "            if conversation_counter > old_conversation_counter+2:\n",
    "                conversation = \"\".join(conversation_buffer)\n",
    "                print(\"starting recommendation\")\n",
    "                print(\"conversation: \", conversation)\n",
    "                recommend(conversation)\n",
    "                print(\"ending recommendation\")\n",
    "                old_conversation_counter=conversation_counter\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "stream_conversation(conversation_buffer,conversation_pause_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
